+++
title = "k8s practice"
description = "the way to kubernetes"
tags = [
    "kubernetes",
    "k8s"
]
date = "2020-08-23"
topics = [
    "kubernetes",
    "k8s"
]
draft = false
toc = true
+++

## Tasks

ç»ƒä¹ è®°å½•ï¼Œ[å®˜æ–¹Tasks](https://kubernetes.io/docs/tasks/)ï¼Œ[ä¸­æ–‡ç‰ˆæœ¬å¯¹ç…§](https://kubernetes.io/zh/docs/tasks/)è·Ÿç€ç»ƒä¹ ä¸€éä¹‹åï¼Œå„ä¸ªæ¦‚å¿µå°±æ¸…æ¥šäº†ã€‚

### æ›´æ–°DaemonSet 

[å¯¹ DaemonSet æ‰§è¡Œæ»šåŠ¨æ›´æ–°](https://kubernetes.io/zh/docs/tasks/manage-daemon/update-daemon-set/)ï¼Œæœ¬åœ°æ§åˆ¶DOçš„é›†ç¾¤ï¼Œé»˜è®¤ä¼šè¶…æ—¶ï¼Œæ— æ³•applyå¯¹è±¡ï¼Œæç¤º

```
E1123 19:37:27.679680     511 request.go:1027] Unexpected error when reading response body: net/http: request canceled (Client.Timeout or context cancellation while reading body)
error: unexpected error when reading response body. Please retry. Original error: net/http: request canceled (Client.Timeout or context cancellation while reading body)
# æˆ–è€… 
Unable to connect to the server: net/http: TLS handshake timeout
```

å¢åŠ è¶…æ—¶æ—¶é—´è®¾ç½®`k apply -f xxx.yaml  --request-timeout="50"`ï¼Œè¿˜å¯ä»¥æŒ‡å®šlogçº§åˆ«`-v=6`ï¼Œå‚è€ƒ[ã€Kubernetesã€‘Client.Timeout or context cancellation while reading bodyã¨ã„ã†ã‚¨ãƒ©ãƒ¼ã®è§£æ¶ˆæ–¹æ³•](https://www.mtioutput.com/entry/kubectl-timeout-error)

- è®¾ç½®DaemonSetå†…å®¹ï¼ŒæŸ¥çœ‹æ›´æ–°ç­–ç•¥`kubectl get ds/fluentd-elasticsearch  -n kube-system -o go-template='{{.spec.updateStrategy.type}}{{"\n"}}'` æŸ¥çœ‹æ–¹å¼ä¹Ÿå¯ä»¥ä¸º `-o yaml`
- å¯¹ RollingUpdate DaemonSet çš„ `.spec.template` çš„ä»»ä½•æ›´æ–°éƒ½å°†è§¦å‘æ»šåŠ¨æ›´æ–°ã€‚ä¾‹å¦‚å¢åŠ äº†cpuå’Œå†…å­˜çš„é™åˆ¶

ä»¥ä¸‹æ–¹å¼éƒ½å¯ä»¥æ›´æ–°â€”â€”
```shell
# å£°æ˜å¼å‘½ä»¤
kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset-update.yaml
# æŒ‡ä»¤å¼å‘½ä»¤ Edit a resource from the default editor.
kubectl edit ds/fluentd-elasticsearch -n kube-system #æ‰“å¼€æ–‡ä»¶ç¼–è¾‘ï¼Œä¿æŒã€‚è¿œç¨‹ä¸å»ºè®®è¿™æ ·æ“ä½œ
# åªæ›´æ–°å®¹å™¨é•œåƒ
kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=quay.io/fluentd_elasticsearch/fluentd:v2.6.0 -n kube-system
```

æ›´æ–°ç»“æœç±»ä¼¼â€”â€”
```
myu@Gebitang:~$ k apply -f fluentd-daemonset-update.yaml
daemonset.apps/fluentd-elasticsearch configured
myu@Gebitang:~$ kubectl rollout status ds/fluentd-elasticsearch -n kube-system
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 1 out of 3 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 1 out of 3 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 2 out of 3 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 2 out of 3 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 2 out of 3 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 2 of 3 updated pods are available...
daemon set "fluentd-elasticsearch" successfully rolled out
```

ä»åå­—ç©ºé—´ä¸­åˆ é™¤ DaemonSetï¼š`kubectl delete ds fluentd-elasticsearch -n kube-system`

### ç®¡ç†å†…å­˜ã€CPUã€é…é¢

[ç®¡ç†å†…å­˜](https://kubernetes.io/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/)ï¼Œæç¤º`looking up service account default-mem-example/default: serviceaccount "default" not found`ï¼Œæ„å‘³ç€æ²¡æœ‰æœåŠ¡è´¦å·(é»˜è®¤ç©ºé—´æœ‰é»˜è®¤çš„è´¦å·ï¼Œä½†æ–°åˆ›å»ºçš„namespaceä¸‹æ²¡æœ‰æœåŠ¡è´¦æˆ·)ï¼Œå¯ä»¥å…ˆæ‰§è¡Œä»»åŠ¡[ä¸ºpodé…ç½®æœåŠ¡è´¦æˆ·](https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-service-account/)

```shell
kubectl create -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: default
  namespace: default-mem-example
EOF
# ç„¶åå†åœ¨å¯¹åº”ç©ºé—´ä¸‹åˆ›å»ºpodå³å¯
```

æ‰§è¡Œåˆ é™¤namespaceçš„å‘½ä»¤å`k delete namespace default-mem-example`ï¼Œä¸€ç›´æ²¡æœ‰è¿”å›ç»“æœï¼ŒCtrl+Cå¼ºåˆ¶ç»“æŸï¼ŒæŸ¥çœ‹çŠ¶æ€æ˜¾ç¤ºä¸º`Terminating`ï¼Œå‚è€ƒ[Namespace "stuck" as Terminating, How I removed it](https://stackoverflow.com/questions/52369247/namespace-stuck-as-terminating-how-i-removed-it)ï¼Œå½“å‰ç©ºé—´ä¸‹è¿˜æœ‰å…¶ä»–èµ„æºï¼Ÿ`kubectl api-resources --verbs=list --namespaced -o name  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n default-mem-example`ï¼Œåˆ é™¤å¯¹åº”çš„èµ„æºåï¼ŒçŠ¶æ€ä¾ç„¶ä¸º`Terminating`

```shell
geb@Gebitang:~$ k delete limitranges --namespace=default-mem-example
error: resource(s) were provided, but no name was specified
geb@Gebitang:~$ k delete limitrange/mem-limit-range --namespace=default-mem-example
limitrange "mem-limit-range" deleted
geb@Gebitang:~$ k delete serviceaccount/default --namespace=default-mem-example
serviceaccount "default" deleted

# æŸ¥çœ‹ç©ºé—´ä¿¡æ¯ k get namespace default-mem-example -o json
```

```json
{
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2021-11-23T07:17:26Z",
        "deletionTimestamp": "2021-11-23T07:55:36Z",
        "labels": {
            "kubernetes.io/metadata.name": "default-mem-example"
        },
        "name": "default-mem-example",
        "resourceVersion": "2161742",
        "uid": "7f0fd528-b681-45f7-887e-49820024333e"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Terminating"
    }
}
```

è°ƒç”¨apiï¼Œå°†finalizerså­—æ®µç½®ç©ºå³å¯ï¼š

```shell
# ä½¿ç”¨proxyï¼Œç„¶åè°ƒç”¨http server api
NAMESPACE=your-rogue-namespace
kubectl proxy &
kubectl get namespace $NAMESPACE -o json |jq '.spec = {"finalizers":[]}' >temp.json
curl -k -H "Content-Type: application/json" -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize

# ç›´æ¥è°ƒç”¨ï¼Œå°†ä¸Šè¿°jsonå†…å®¹çš„finalizersæ•°ç»„ç½®ä¸ºç©ºï¼Œä¿å­˜ä¸º default-mem-example.jsonï¼Œ è°ƒç”¨
 k replace --raw "/api/v1/namespaces/default-mem-example/finalize" -f default-mem-example.json
# å†æŸ¥çœ‹ç©ºé—´æ—¶ï¼Œæ­¤ç©ºé—´è¢«å½»åº•åˆ é™¤
```

å…¶ä»–èµ„æºç±»ä¼¼


## start over 

### çº¯æ‰‹å·¥æ­å»ºç¯å¢ƒ

ç›®å‰åªæœ‰ä¸€å°ubuntu 20.04çš„serveræœºå™¨ã€‚

å…ˆæ”¶å·¥[å®‰è£…dockerç¯å¢ƒ](https://docs.docker.com/engine/install/ubuntu/)ï¼Œä»[ä¸‹è½½é¡µé¢](https://download.docker.com/linux/ubuntu/dists/focal/pool/stable/amd64/)ä¸‹è½½docker-ce docker-ce-cli containerd.io

**containerd.io**Â : daemonÂ [containerd](https://containerd.io/). ItÂ [works](https://containerd.io/docs/getting-started/)Â independently on the docker packages, and it is required by the docker packages.

> containerd is available as a daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond.

**docker-ce-cli**Â : command line interface for docker engine, community edition

**docker-ce**Â :Â [docker](https://docs.docker.com/get-docker/)Â engine, community edition. Requires docker-ce-cli.

>containerd.io_1.4.12-1_amd64.deb 2021-12-11 23:03:31 22.6 MiB
>docker-ce-cli_20.10.12~3-0~ubuntu-focal_amd64.deb  2021-12-13 14:38:46 38.8 MiB
>docker-ce_20.10.12~3-0~ubuntu-focal_amd64.deb  2021-12-13 14:38:48 20.3 MiB

æ³¨æ„å®‰è£…é¡ºåºâ€”â€”å®‰è£…å®Œæˆåä¼šè‡ªåŠ¨åˆ›å»ºå¯¹åº”çš„systemd services

```
âœ  deps sudo dpkg -i containerd.io_1.4.12-1_amd64.deb
[sudo] password for geb:
Selecting previously unselected package containerd.io.
(Reading database ... 109132 files and directories currently installed.)
Preparing to unpack containerd.io_1.4.12-1_amd64.deb ...
Unpacking containerd.io (1.4.12-1) ...
Setting up containerd.io (1.4.12-1) ...
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service â†’ /lib/systemd/system/containerd.service.
Processing triggers for man-db (2.9.1-1) ...
âœ  deps sudo dpkg -i docker-ce_20.10.12_3-0_ubuntu-focal_amd64.deb
Selecting previously unselected package docker-ce.
(Reading database ... 109148 files and directories currently installed.)
Preparing to unpack docker-ce_20.10.12_3-0_ubuntu-focal_amd64.deb ...
Unpacking docker-ce (5:20.10.12~3-0~ubuntu-focal) ...
dpkg: dependency problems prevent configuration of docker-ce:
 docker-ce depends on docker-ce-cli; however:
  Package docker-ce-cli is not installed.

dpkg: error processing package docker-ce (--install):
 dependency problems - leaving unconfigured
Processing triggers for systemd (245.4-4ubuntu3.15) ...
Errors were encountered while processing:
 docker-ce
âœ  deps sudo dpkg -i docker-ce-cli_20.10.12_3-0_ubuntu-focal_amd64.deb
Selecting previously unselected package docker-ce-cli.
(Reading database ... 109160 files and directories currently installed.)
Preparing to unpack docker-ce-cli_20.10.12_3-0_ubuntu-focal_amd64.deb ...
Unpacking docker-ce-cli (5:20.10.12~3-0~ubuntu-focal) ...
Setting up docker-ce-cli (5:20.10.12~3-0~ubuntu-focal) ...
Processing triggers for man-db (2.9.1-1) ...
âœ  deps sudo dpkg -i docker-ce_20.10.12_3-0_ubuntu-focal_amd64.deb
(Reading database ... 109358 files and directories currently installed.)
Preparing to unpack docker-ce_20.10.12_3-0_ubuntu-focal_amd64.deb ...
Unpacking docker-ce (5:20.10.12~3-0~ubuntu-focal) over (5:20.10.12~3-0~ubuntu-focal) ...
Setting up docker-ce (5:20.10.12~3-0~ubuntu-focal) ...
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service â†’ /lib/systemd/system/docker.service.
Created symlink /etc/systemd/system/sockets.target.wants/docker.socket â†’ /lib/systemd/system/docker.socket.
Processing triggers for systemd (245.4-4ubuntu3.15) ...
```

é…ç½®å®‰è£…æº(æœ¬é¡µé¢åç»­å†…å®¹)åï¼Œ[å®‰è£… kubeadmã€kubelet å’Œ kubectl](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)

- kubeadmï¼šç”¨æ¥åˆå§‹åŒ–é›†ç¾¤çš„æŒ‡ä»¤ã€‚
- kubeletï¼šåœ¨é›†ç¾¤ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä¸Šç”¨æ¥å¯åŠ¨ Pod å’Œå®¹å™¨ç­‰ã€‚
- kubectlï¼šç”¨æ¥ä¸é›†ç¾¤é€šä¿¡çš„å‘½ä»¤è¡Œå·¥å…·ã€‚

```
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

#ç®—ä¸Šä»¥æ¥æ–‡ä»¶ï¼Œå®é™…ä¸Šå®‰è£…äº†8ä¸ªåº”ç”¨
conntrack cri-tools ebtables kubeadm kubectl kubelet kubernetes-cni socat 
```

æ‰§è¡Œåˆå§‹åŒ–åŠ¨ä½œ`sudo kubeadm init --v=5`å¯ä»¥çœ‹åˆ°æ›´è¯¦ç»†çš„å®‰è£…ä¿¡æ¯ï¼Œæ ¹æ®logä¿¡æ¯å¯ä»¥çœ‹åˆ°å¤§è‡´æ‰§è¡Œäº†å“ªäº›åŠ¨ä½œâ€”â€”

```
âœ sudo kubeadm init --v=5
I0116 11:17:10.928410   83074 initconfiguration.go:117] detected and using CRI socket: /var/run/dockershim.sock
I0116 11:17:10.928585   83074 interface.go:432] Looking for default routes with IPv4 addresses
I0116 11:17:10.928591   83074 interface.go:437] Default route transits interface "eno1"
I0116 11:17:10.928723   83074 interface.go:209] Interface eno1 is up
I0116 11:17:10.928767   83074 interface.go:257] Interface "eno1" has 4 addresses :[192.168.10.101/24 fd0d:7276:3cac::506/128 fd0d:7276:3cac:0:6202:92ff:fe39:bf83/64 fe80::6202:92ff:fe39:bf83/64].
I0116 11:17:10.928790   83074 interface.go:224] Checking addr  192.168.10.101/24.
I0116 11:17:10.928806   83074 interface.go:231] IP found 192.168.10.101
I0116 11:17:10.928828   83074 interface.go:263] Found valid IPv4 address 192.168.10.101 for interface "eno1".
I0116 11:17:10.928834   83074 interface.go:443] Found active IP 192.168.10.101
I0116 11:17:10.928859   83074 kubelet.go:217] the value of KubeletConfiguration.cgroupDriver is empty; setting it to "systemd"
I0116 11:17:10.931755   83074 version.go:186] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.txt
[init] Using Kubernetes version: v1.23.1
[preflight] Running pre-flight checks
I0116 11:17:11.640595   83074 checks.go:578] validating Kubernetes and kubeadm version
I0116 11:17:11.640657   83074 checks.go:171] validating if the firewall is enabled and active
I0116 11:17:11.654060   83074 checks.go:206] validating availability of port 6443
I0116 11:17:11.654160   83074 checks.go:206] validating availability of port 10259
I0116 11:17:11.654181   83074 checks.go:206] validating availability of port 10257
I0116 11:17:11.654200   83074 checks.go:283] validating the existence of file /etc/kubernetes/manifests/kube-apiserver.yaml
I0116 11:17:11.654217   83074 checks.go:283] validating the existence of file /etc/kubernetes/manifests/kube-controller-manager.yaml
I0116 11:17:11.654224   83074 checks.go:283] validating the existence of file /etc/kubernetes/manifests/kube-scheduler.yaml
I0116 11:17:11.654232   83074 checks.go:283] validating the existence of file /etc/kubernetes/manifests/etcd.yaml
I0116 11:17:11.654239   83074 checks.go:433] validating if the connectivity type is via proxy or direct
I0116 11:17:11.654253   83074 checks.go:472] validating http connectivity to first IP address in the CIDR
I0116 11:17:11.654266   83074 checks.go:472] validating http connectivity to first IP address in the CIDR
I0116 11:17:11.654271   83074 checks.go:107] validating the container runtime
I0116 11:17:11.722204   83074 checks.go:133] validating if the "docker" service is enabled and active
I0116 11:17:11.772580   83074 checks.go:332] validating the contents of file /proc/sys/net/bridge/bridge-nf-call-iptables
I0116 11:17:11.772659   83074 checks.go:332] validating the contents of file /proc/sys/net/ipv4/ip_forward
I0116 11:17:11.772693   83074 checks.go:654] validating whether swap is enabled or not
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0116 11:17:11.772789   83074 checks.go:373] validating the presence of executable conntrack
I0116 11:17:11.772831   83074 checks.go:373] validating the presence of executable ip
I0116 11:17:11.772854   83074 checks.go:373] validating the presence of executable iptables
I0116 11:17:11.772877   83074 checks.go:373] validating the presence of executable mount
I0116 11:17:11.772911   83074 checks.go:373] validating the presence of executable nsenter
I0116 11:17:11.772960   83074 checks.go:373] validating the presence of executable ebtables
I0116 11:17:11.772987   83074 checks.go:373] validating the presence of executable ethtool
I0116 11:17:11.773018   83074 checks.go:373] validating the presence of executable socat
I0116 11:17:11.773048   83074 checks.go:373] validating the presence of executable tc
I0116 11:17:11.773077   83074 checks.go:373] validating the presence of executable touch
I0116 11:17:11.773100   83074 checks.go:521] running all checks
I0116 11:17:11.856347   83074 checks.go:404] checking whether the given node name is valid and reachable using net.LookupHost
I0116 11:17:11.856366   83074 checks.go:620] validating kubelet version
I0116 11:17:11.900767   83074 checks.go:133] validating if the "kubelet" service is enabled and active
I0116 11:17:11.919129   83074 checks.go:206] validating availability of port 10250
I0116 11:17:11.919197   83074 checks.go:206] validating availability of port 2379
I0116 11:17:11.919230   83074 checks.go:206] validating availability of port 2380
I0116 11:17:11.919259   83074 checks.go:246] validating the existence and emptiness of directory /var/lib/etcd
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0116 11:17:11.919362   83074 checks.go:842] using image pull policy: IfNotPresent
I0116 11:17:11.956674   83074 checks.go:859] pulling: k8s.gcr.io/kube-apiserver:v1.23.1
I0116 11:18:27.630052   83074 checks.go:859] pulling: k8s.gcr.io/kube-controller-manager:v1.23.1
I0116 11:19:43.197056   83074 checks.go:859] pulling: k8s.gcr.io/kube-scheduler:v1.23.1
I0116 11:20:58.742768   83074 checks.go:859] pulling: k8s.gcr.io/kube-proxy:v1.23.1
I0116 11:22:14.285969   83074 checks.go:859] pulling: k8s.gcr.io/pause:3.6
I0116 11:23:29.802417   83074 checks.go:859] pulling: k8s.gcr.io/etcd:3.5.1-0
I0116 11:24:45.383728   83074 checks.go:859] pulling: k8s.gcr.io/coredns/coredns:v1.8.6

ç”±äºä¸Šèµ°çš„gcrï¼ˆGoogle Cloud Container Registry)ï¼Œæ‰€ä»¥åé¢å°±è¶…æ—¶å¤±è´¥äº†


```

ä¸‹è½½clashï¼Œ`gunzip clashxx.gz`è§£å‹ï¼›ä¸‹è½½é…ç½®æ–‡ä»¶ï¼›[ä¸‹è½½mmdb](https://github.com/Dreamacro/clash/issues/854)ï¼›å¯åŠ¨` ./clash -d conf -f clash.yml`èµ°ä»£ç†ä¸‹è½½é•œåƒï¼›å…³é—­swap `sudo swapoff -a` è¿˜æ˜¯æç¤ºè¶…æ—¶ã€‚[å‚è€ƒ](https://zhuanlan.zhihu.com/p/46341911)

æ‰§è¡Œ`kubeadm config image list`è·å–éœ€è¦å®‰è£…çš„é•œåƒåˆ—è¡¨ï¼Œç„¶åä»å›½å†…ç«™ç‚¹æ‹‰å–

```
#!/bin/bash

images=(  # ä¸‹é¢çš„é•œåƒåº”è¯¥å»é™¤"k8s.gcr.io/"çš„å‰ç¼€ï¼Œç‰ˆæœ¬æ¢æˆä¸Šé¢è·å–åˆ°çš„ç‰ˆæœ¬
    kube-apiserver:v1.23.1
kube-controller-manager:v1.23.1
kube-scheduler:v1.23.1
kube-proxy:v1.23.1
pause:3.6
etcd:3.5.1-0
coredns:v1.8.6
)

for imageName in ${images[@]} ; do
sudo docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
   sudo  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
   sudo  docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
```

ä¸‹è½½å®Œæˆåï¼Œè¿˜éœ€è¦æŠŠcorednsçš„tagé‡æ–°å‘½åä¸€ä¸‹ï¼š

```
#å› ä¸ºç›´æ¥ä¸‹è½½ coredns/coredns:v1.8.6æ—¶ä¼šæ‰¾ä¸åˆ°ï¼Œéœ€è¦ä¸‹è½½coredns:v1.8.6ï¼Œä½†kubeadméœ€è¦çš„æ˜¯k8s.gcr.io/coredns/coredns:v1.8.6 
sudo docker tag k8s.gcr.io/coredns:v1.8.6 k8s.gcr.io/coredns/coredns:v1.8.6
sudo docker rmi k8s.gcr.io/coredns:v1.8.6
```

é•œåƒOKï¼Œå¥åº·æ£€æŸ¥å¤±è´¥ã€‚çš„ç¡®æ˜¯kubeletè¢«é€€å‡ºã€‚ 

```
I0116 12:34:31.850670   95794 checks.go:851] image exists: k8s.gcr.io/kube-apiserver:v1.23.1
I0116 12:34:31.878709   95794 checks.go:851] image exists: k8s.gcr.io/kube-controller-manager:v1.23.1
I0116 12:34:31.905295   95794 checks.go:851] image exists: k8s.gcr.io/kube-scheduler:v1.23.1
I0116 12:34:31.933910   95794 checks.go:851] image exists: k8s.gcr.io/kube-proxy:v1.23.1
I0116 12:34:31.961095   95794 checks.go:851] image exists: k8s.gcr.io/pause:3.6
I0116 12:34:31.989404   95794 checks.go:851] image exists: k8s.gcr.io/etcd:3.5.1-0
I0116 12:34:32.015336   95794 checks.go:851] image exists: k8s.gcr.io/coredns/coredns:v1.8.6
[certs] Using certificateDir folder "/etc/kubernetes/pki"
I0116 12:34:32.015391   95794 certs.go:112] creating a new certificate authority for ca
[certs] Generating "ca" certificate and key
I0116 12:34:32.333727   95794 certs.go:522] validating certificate period for ca certificate
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local lee] and IPs [10.96.0.1 192.168.10.101]
[certs] Generating "apiserver-kubelet-client" certificate and key
I0116 12:34:32.801641   95794 certs.go:112] creating a new certificate authority for front-proxy-ca
[certs] Generating "front-proxy-ca" certificate and key
I0116 12:34:32.918912   95794 certs.go:522] validating certificate period for front-proxy-ca certificate
[certs] Generating "front-proxy-client" certificate and key
I0116 12:34:33.148388   95794 certs.go:112] creating a new certificate authority for etcd-ca
[certs] Generating "etcd/ca" certificate and key
I0116 12:34:33.468493   95794 certs.go:522] validating certificate period for etcd/ca certificate
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [lee localhost] and IPs [192.168.10.101 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [lee localhost] and IPs [192.168.10.101 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
I0116 12:34:34.288630   95794 certs.go:78] creating new public/private key files for signing service account users
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0116 12:34:34.899330   95794 kubeconfig.go:103] creating kubeconfig file for admin.conf
[kubeconfig] Writing "admin.conf" kubeconfig file
I0116 12:34:35.094669   95794 kubeconfig.go:103] creating kubeconfig file for kubelet.conf
[kubeconfig] Writing "kubelet.conf" kubeconfig file
I0116 12:34:35.256831   95794 kubeconfig.go:103] creating kubeconfig file for controller-manager.conf
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0116 12:34:35.520261   95794 kubeconfig.go:103] creating kubeconfig file for scheduler.conf
[kubeconfig] Writing "scheduler.conf" kubeconfig file
I0116 12:34:35.645722   95794 kubelet.go:65] Stopping the kubelet
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
I0116 12:34:36.020442   95794 manifests.go:99] [control-plane] getting StaticPodSpecs
I0116 12:34:36.020618   95794 certs.go:522] validating certificate period for CA certificate
I0116 12:34:36.020674   95794 manifests.go:125] [control-plane] adding volume "ca-certs" for component "kube-apiserver"
I0116 12:34:36.020680   95794 manifests.go:125] [control-plane] adding volume "etc-ca-certificates" for component "kube-apiserver"
I0116 12:34:36.020685   95794 manifests.go:125] [control-plane] adding volume "etc-pki" for component "kube-apiserver"
I0116 12:34:36.020690   95794 manifests.go:125] [control-plane] adding volume "k8s-certs" for component "kube-apiserver"
I0116 12:34:36.020694   95794 manifests.go:125] [control-plane] adding volume "usr-local-share-ca-certificates" for component "kube-apiserver"
I0116 12:34:36.020699   95794 manifests.go:125] [control-plane] adding volume "usr-share-ca-certificates" for component "kube-apiserver"
I0116 12:34:36.024333   95794 manifests.go:154] [control-plane] wrote static Pod manifest for component "kube-apiserver" to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
I0116 12:34:36.024368   95794 manifests.go:99] [control-plane] getting StaticPodSpecs
I0116 12:34:36.024548   95794 manifests.go:125] [control-plane] adding volume "ca-certs" for component "kube-controller-manager"
I0116 12:34:36.024562   95794 manifests.go:125] [control-plane] adding volume "etc-ca-certificates" for component "kube-controller-manager"
I0116 12:34:36.024570   95794 manifests.go:125] [control-plane] adding volume "etc-pki" for component "kube-controller-manager"
I0116 12:34:36.024578   95794 manifests.go:125] [control-plane] adding volume "flexvolume-dir" for component "kube-controller-manager"
I0116 12:34:36.024585   95794 manifests.go:125] [control-plane] adding volume "k8s-certs" for component "kube-controller-manager"
I0116 12:34:36.024593   95794 manifests.go:125] [control-plane] adding volume "kubeconfig" for component "kube-controller-manager"
I0116 12:34:36.024600   95794 manifests.go:125] [control-plane] adding volume "usr-local-share-ca-certificates" for component "kube-controller-manager"
I0116 12:34:36.024608   95794 manifests.go:125] [control-plane] adding volume "usr-share-ca-certificates" for component "kube-controller-manager"
I0116 12:34:36.025297   95794 manifests.go:154] [control-plane] wrote static Pod manifest for component "kube-controller-manager" to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[control-plane] Creating static Pod manifest for "kube-scheduler"
I0116 12:34:36.025321   95794 manifests.go:99] [control-plane] getting StaticPodSpecs
I0116 12:34:36.025475   95794 manifests.go:125] [control-plane] adding volume "kubeconfig" for component "kube-scheduler"
I0116 12:34:36.025888   95794 manifests.go:154] [control-plane] wrote static Pod manifest for component "kube-scheduler" to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0116 12:34:36.026504   95794 local.go:65] [etcd] wrote Static Pod manifest for a local etcd member to "/etc/kubernetes/manifests/etcd.yaml"
I0116 12:34:36.026515   95794 waitcontrolplane.go:91] [wait-control-plane] Waiting for the API server to be healthy
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused.

	Unfortunately, an error has occurred:
		timed out waiting for the condition

	This error is likely caused by:
		- The kubelet is not running
		- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

	If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
		- 'systemctl status kubelet'
		- 'journalctl -xeu kubelet'

	Additionally, a control plane component may have crashed or exited when started by the container runtime.
	To troubleshoot, list all containers using your preferred container runtimes CLI.

	Here is one example how you may list all Kubernetes containers running in docker:
		- 'docker ps -a | grep kube | grep -v pause'
		Once you have found the failing container, you can inspect its logs with:
		- 'docker logs CONTAINERID'

couldn't initialize a Kubernetes cluster
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/init.runWaitControlPlanePhase
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/init/waitcontrolplane.go:118
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:234
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:421
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdInit.func1
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/init.go:153
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:856
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:974
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:902
k8s.io/kubernetes/cmd/kubeadm/app.Run
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50
main.main
	_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25
runtime.main
	/usr/local/go/src/runtime/proc.go:255
runtime.goexit
	/usr/local/go/src/runtime/asm_amd64.s:1581
error execution phase wait-control-plane
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:235
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:421
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdInit.func1
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/init.go:153
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:856
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:974
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:902
k8s.io/kubernetes/cmd/kubeadm/app.Run
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50
main.main
	_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25
runtime.main
	/usr/local/go/src/runtime/proc.go:255
runtime.goexit
	/usr/local/go/src/runtime/asm_amd64.s:1581
âœ  ~
```

[kubeadm init shows kubelet isn't running or healthy](https://stackoverflow.com/questions/52119985/kubeadm-init-shows-kubelet-isnt-running-or-healthy)



### wsl2 æ—¶é—´åŒæ­¥

æŒ‰ç…§[Kubernetes Tutorial - Step by Step Introduction to Basic Concepts](https://auth0.com/blog/kubernetes-tutorial-step-by-step-introduction-to-basic-concepts/)ä»‹ç»åœ¨DOä¸Šåˆ›å»ºå…è´¹çš„k8sé›†ç¾¤ï¼Œåœ¨wsl2é‡Œè¿›è¡Œè®¿é—®æ—¶æç¤ºé”™è¯¯ï¼š`Unable to connect to the server: x509: certificate has expired or is not yet valid: current time 2021-11-09T02:59:51+08:00 is before 2021-11-09T07:47:25Z`æŸ¥çœ‹æœ¬åœ°æ—¶é—´ï¼Œæ˜¾ç¤ºæ¯”å®é™…å®é™…æ…¢

æ‰§è¡Œ`sudo hwclock -s `åŒæ­¥æ—¶é—´ä¾ç„¶æ²¡æœ‰å˜åŒ–ï¼Œå®˜æ–¹[issue 4149](https://github.com/microsoft/WSL/issues/4149)ä¸­æä¾›äº†ä¸´æ—¶è§£å†³æ–¹æ¡ˆâ€”â€”netdateã€‚ç«‹å³ç”Ÿæ•ˆï¼Œä¸å½±å“ä»»ä½•è¿›ç¨‹

```
# æŒ‰ç…§ntpdate set the date and time via  Network Time Protocol (NTP)
sudo apt install ntpdate
# åŒæ­¥æ—¶é—´
sudo ntpdate -sb time.nist.gov
# æˆ–æ‰§è¡Œ
sudo ntpdate time.windows.com

myu@Gebitang:~$ sudo ntpdate time.windows.com
 9 Nov 16:09:43 ntpdate[1727]: step time server 52.231.114.183 offset 46499.066385 sec
```

åŒæ­¥æ—¶é—´ä¹‹åï¼Œå¯ä»¥è·Ÿé›†ç¾¤æ­£å¸¸äº¤äº’

### Kubernetes Tutorial

ä¸Šé¢æåˆ°çš„æ–‡ç« æœ‰ç‚¹è€ï¼Œé‡åˆ°çš„é—®é¢˜è®°å½•ä¸€ä¸‹â€”â€”

- é€šè¿‡promote codeæ³¨å†ŒDOéœ€è¦æ‰‹åŠ¨æäº¤ä¸ªticketï¼Œå¦åˆ™æ— æ³•åˆ›å»ºk8sé›†ç¾¤ï¼Œæç¤ºdropletè¶…é™
- éƒ¨ç½²`NGINX ingress controller`çš„yamlæ–‡ä»¶åœ°å€å·²å¤±æ•ˆï¼Œå‚è€ƒ[ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/#digital-ocean)ä¸­çš„è¯´æ˜ï¼Œä½¿ç”¨`https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.4/deploy/static/provider/do/deploy.yaml`æ–‡ä»¶

```
myu@Gebitang:~$ k get pods -n ingress-nginx
NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-n2dgj        0/1     Completed   0          87s
ingress-nginx-admission-patch-jldqd         0/1     Completed   2          86s
ingress-nginx-controller-5c8d66c76d-fgvpb   1/1     Running     0          90s
```

- æœ€ç»ˆéƒ¨ç½²å®Œserviceä¹‹åï¼Œè·å–å¤–ç½‘ipçš„æ­¥éª¤ä¹Ÿå¤±æ•ˆäº†ï¼Œé€šè¿‡`k get svc -n ingress-nginx`å¯ä»¥çœ‹åˆ°å¤–ç½‘ipåœ°å€ï¼Œå°½ç®¡è®¿é—®æœåŠ¡æ—¶æç¤º404ï¼Œè‡³å°‘è¡¨ç¤ºnginxå¯åŠ¨èµ·æ¥äº†

### K8Sä¸­çš„CRDå¼€å‘

CRD: `CustomResourceDefinition`, Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.

[2021-05-04ç‰ˆæœ¬](http://bingerambo.com/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91)æµç¨‹ã€‚ä¾èµ–[code-generator](https://github.com/kubernetes/code-generator)ï¼Œå®˜æ–¹ç¤ºä¾‹[sample-controller](https://github.com/kubernetes/sample-controller)ï¼Œä¹Ÿå¯ä»¥å‚è€ƒ[K8s Clientä»£ç è‡ªåŠ¨ç”Ÿæˆ](http://ljchen.net/2019/06/14/K8s-client%E4%BB%A3%E7%A0%81%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90/)

ä½¿ç”¨ä»£ç ç”Ÿæˆå™¨ç”Ÿæˆçš„apiå…¼å®¹æ ‡å‡†çš„k8s apiï¼Œæ“ä½œåŠ¨ä½œç›¸åŒã€‚


### setup with dashboard

[download binary](https://www.downloadkubernetes.com/)

[Install kubectl on Linux](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)

å…³äºUIç®¡ç†ä½¿ç”¨çš„[kubernetes/dashboard](https://github.com/kubernetes/dashboard/tree/master/docs)ï¼Œå‚è€ƒå…·ä½“çš„[doc: creating sample user](https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md)

å…³äºç”¨æˆ·çš„æ­¥éª¤â€”â€”

- åŸºäºRBACçš„æ–¹å¼åˆ›å»º`ServiceAccount`
- å°†åˆ›å»ºçš„SAè¿›è¡Œ`CluserRoleBinding`

ç¤ºä¾‹ä¸­å¦‚æœå°†namespaceæŒ‡å®šä¸º`cluser-admin`ï¼Œå°†æ‹¥æœ‰ç®¡ç†å‘˜æƒé™ã€‚(é€šè¿‡`k get clusterroles`æŸ¥çœ‹é›†ç¾¤ä¸­éƒ½æœ‰å“ªäº›è§’è‰²)

```
chmod +x kubectl
mkdir -p ~/.local/bin/kubectl
mv ./kubectl ~/.local/bin/kubectl
# and then add ~/.local/bin/kubectl to $PATH
```

è‡ªåŠ¨å®ŒæˆEnable kubectl autocompletion

```shell
source <(kubectl completion bash)
alias k=kubectl
complete -F __start_kubectl k

#shellç”Ÿæ•ˆ
echo 'source <(kubectl completion bash)' >>~/.bashrc
echo 'alias k=kubectl' >>~/.bashrc
echo 'complete -F __start_kubectl k' >>~/.bashrc
```

ä½¿ç”¨`kind`åˆ›å»ºé›†ç¾¤ [quick start](https://kind.sigs.k8s.io/docs/user/quick-start/)
```shell
# åˆ›å»ºä¸€ä¸ª3èŠ‚ç‚¹é›†ç¾¤çš„é…ç½®æ–‡ä»¶
cat << EOF > kind-3nodes.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF
# ä½¿ç”¨é…ç½®æ–‡ä»¶åˆ›å»ºæ–°çš„é›†ç¾¤
kind create cluster --name k3s --config ./kind-3nodes.yaml
# è·å–é›†ç¾¤èŠ‚ç‚¹
kubectl get nodes
# åˆ é™¤é›†ç¾¤
kind delete cluster --name k3s
```
### WSL+Dockerçš„é™åˆ¶

[WSL+Docker: Kubernetes on the Windows Desktop](https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/) 

ä»WSL2é‡Œçš„ubuntu20.04é•œåƒé‡Œä½¿ç”¨kindåˆ›å»ºçš„k8s clusterï¼Œæ— æ³•clusterçš„nodeçš„ipï¼Œç½‘ç»œä¸é€šã€‚â€”â€”è¿™çœ‹èµ·æ¥æ˜¯ä¸ªå·²çŸ¥çš„é—®é¢˜ï¼š[Networking features in Docker Desktop for Windows](https://docs.docker.com/desktop/windows/networking/)â€”â€”

- **There is no docker0 bridge on Windows**  
Because of the way networking is implemented in Docker Desktop for Windows, you cannot see a docker0 interface on the host. This interface is actually within the virtual machine.

- **I cannot ping my containers**  
Docker Desktop for Windows canâ€™t route traffic to Linux containers. However, you can ping the Windows containers.

- **Per-container IP addressing is not possible** 
The docker (Linux) bridge network is not reachable from the Windows host. However, it works with Windows containers.


è·Ÿç€[æ¼”ç¤º](https://www.qikqiak.com/post/deploy-k8s-on-win-use-wsl2)ç›´åˆ°éƒ¨ç½²ä¸‰ä¸ªèŠ‚ç‚¹çš„clusteréƒ½æ­£å¸¸ï¼Œå®‰è£…ä¸€ä¸ª Kubernetes Dashboardæ—¶(`kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.1/aio/deploy/recommended.yaml`)ï¼Œ`kubectl get all -n kubernetes-dashboard`å‘½ä»¤æ˜¾ç¤ºï¼Œä¸€ç›´å¤„äº"ContainerCreating"çŠ¶æ€ã€‚æ„Ÿè§‰åº”è¯¥æ˜¯æˆ‘æœ¬åœ°ç½‘ç»œåŸå› ï¼Ÿ [issue 2863](https://github.com/kubernetes/dashboard/issues/2863)


å¤šä¸ªé…ç½®æ–‡ä»¶åˆ‡æ¢[Configure Access to Multiple Clusters](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/)

```shell
# online.config.yaml for pro, dev.config.yaml for dev
kubectl config --kubeconfig=online.config.yaml view

# export
export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config
export KUBECONFIG_SAVED=$KUBECONFIG
kubectl config view
# clean up 
export KUBECONFIG=$KUBECONFIG_SAVED
```

```
geb@Gebitang:~$ kubectl get all -n kubernetes-dashboard
NAME                                             READY   STATUS              RESTARTS   AGE
pod/dashboard-metrics-scraper-5594697f48-wgpst   0/1     ContainerCreating   0          14m
pod/kubernetes-dashboard-bc8f4fc6f-f2mqg         0/1     ContainerCreating   0          14m

NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/dashboard-metrics-scraper   ClusterIP   10.96.21.147   <none>        8000/TCP   14m
service/kubernetes-dashboard        ClusterIP   10.96.173.37   <none>        443/TCP    14m

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/dashboard-metrics-scraper   0/1     1            0           14m
deployment.apps/kubernetes-dashboard        0/1     1            0           14m

NAME                                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/dashboard-metrics-scraper-5594697f48   1         1         0       14m
replicaset.apps/kubernetes-dashboard-bc8f4fc6f         1         1         0       14m

#  kubectl get pods --all-namespaces
NAMESPACE              NAME                                                      READY   STATUS              RESTARTS   AGE
kube-system            coredns-558bd4d5db-57bh5                                  1/1     Running             0          16m
kube-system            coredns-558bd4d5db-sfsfp                                  1/1     Running             0          16m
kube-system            etcd-wslkindmultinodes-control-plane                      1/1     Running             0          17m
kube-system            kindnet-2k9fg                                             1/1     Running             0          16m
kube-system            kindnet-ssl4j                                             1/1     Running             0          16m
kube-system            kindnet-z5mgq                                             1/1     Running             0          16m
kube-system            kube-apiserver-wslkindmultinodes-control-plane            1/1     Running             0          17m
kube-system            kube-controller-manager-wslkindmultinodes-control-plane   1/1     Running             1          17m
kube-system            kube-proxy-b4r8b                                          1/1     Running             0          16m
kube-system            kube-proxy-mwlms                                          1/1     Running             0          16m
kube-system            kube-proxy-smhm5                                          1/1     Running             0          16m
kube-system            kube-scheduler-wslkindmultinodes-control-plane            1/1     Running             1          17m
kubernetes-dashboard   dashboard-metrics-scraper-5594697f48-wgpst                0/1     ContainerCreating   0          15m
kubernetes-dashboard   kubernetes-dashboard-bc8f4fc6f-f2mqg                      0/1     ContainerCreating   0          15m
local-path-storage     local-path-provisioner-547f784dff-dngw5                   1/1     Running             0          16m
```

éœ€è¦å…ˆå®‰è£…ç½‘ç»œâ€”â€” 

[å‚è€ƒè¿™ä¸ªcomment](https://github.com/kubernetes/dashboard/issues/2863#issuecomment-368785304)

```
# å¯ä»¥å°†urlå†…å®¹ä¿å­˜åˆ°æ–‡ä»¶è¿›è¡Œæ‰§è¡Œ
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml
```

æ‰§è¡Œä¹‹åï¼Œ`kubectl get all -n kubernetes-dashboard`å¯ä»¥çœ‹åˆ°å¯¹åº”çš„podéƒ½æ­£å¸¸è¿è¡Œèµ·æ¥äº†

```
geb@Gebitang:~$ kubectl get all -n kubernetes-dashboard
NAME                                             READY   STATUS    RESTARTS   AGE
pod/dashboard-metrics-scraper-5594697f48-wgpst   1/1     Running   0          33m
pod/kubernetes-dashboard-bc8f4fc6f-f2mqg         1/1     Running   0          33m

NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/dashboard-metrics-scraper   ClusterIP   10.96.21.147   <none>        8000/TCP   33m
service/kubernetes-dashboard        ClusterIP   10.96.173.37   <none>        443/TCP    33m

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/dashboard-metrics-scraper   1/1     1            1           33m
deployment.apps/kubernetes-dashboard        1/1     1            1           33m

NAME                                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/dashboard-metrics-scraper-5594697f48   1         1         1       33m
replicaset.apps/kubernetes-dashboard-bc8f4fc6f         1         1         1       33m
```


proxyå¯åŠ¨åï¼Œä½¿ç”¨å¦‚ä¸‹æ–¹å¼è·å–tokenï¼Œ
```
# åˆ›å»ºä¸€ä¸ªæ–°çš„ ServiceAccount
kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
EOF
# å°†ä¸Šé¢çš„ SA ç»‘å®šåˆ°ç³»ç»Ÿçš„ cluster-admin è¿™ä¸ªé›†ç¾¤è§’è‰²ä¸Š
kubectl apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF

# ----------actual result-------
geb@Gebitang:~$ kubectl apply -f - <<EOF
> apiVersion: v1
> kind: ServiceAccount
etadata:> metadata:
>   name: admin-user
namespac>   namespace: kubernetes-dashboard
> EOF
serviceaccount/admin-user created
geb@Gebitang:~$ kubectl apply -f - <<EOF
iVersio> apiVersion: rbac.authorization.k8s.io/v1
> kind: ClusterRoleBinding
> metadata:
>   name: admin-user
> roleRef:
>   apiGroup: rbac.authorization.k8s.io
>   kind: ClusterRole
>   name: cluster-admin
> subjects:
> - kind: ServiceAccount
>   name: admin-user
>   namespace: kubernetes-dashboard
> EOF
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
geb@Gebitang:~$
geb@Gebitang:~$ kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
Name:         admin-user-token-d4mkv
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: b66a6322-81a3-4bec-af6a-c329c4702ef8

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1066 bytes
namespace:  20 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjdnaktTQ2Nuc1RfbGhWaWs3NHYtRzF2VFhIU2ZTX3g4aFBHUkNfYkROOU0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWQ0bWt2Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiNjZhNjMyMi04MWEzLTRiZWMtYWY2YS1jMzI5YzQ3MDJlZjgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.pSiZrLue_t9R8qXN7Dm6jaOblVnLr_qI0NvkL25N3TB78rvR3O2zIzZ0cJ5ylvl7hdxIRXoKQxZUvY8cCeFRosBdGp_gaevd_33vlLNfcj0UM3x_9IQeS4cqPlI0EkWcrVXbPbQQyCLKBSgaaCNQrijtiQXUgaK2eLh8FCmvFIIE5U2vs-GqlmTy1YNpGr28WjtML-bFvQIsL7BpZieWzBNb7VfnsNMRPjpmUUE6iurVApwZSNTesFcKQP-dW7trN0B8hI7SllRMN64rfUnNwZX58eI5esgicQ2STJ64WkqTRYbGs1up__Iz_xvoRYXQq4bN_SSVnCnLOtf8jpVuyA
geb@Gebitang:~$
```

è®¿é—®`http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/`é€‰æ‹©tokenæ–¹å¼ï¼Œå°†ä¸Šé¢è·å–åˆ°çš„tokenè¾“å…¥åï¼Œå°±å¯ä»¥çœ‹åˆ°UIé¡µé¢

åˆ é™¤é›†ç¾¤â€”â€”

```
# æŸ¥æ‰¾é›†ç¾¤
kind get cluster 
# åˆ é™¤ç°åœ¨çš„é›†ç¾¤
kind delete cluster --name wslkindmultinodes
```
## archive 
### æœ¯è¯­åˆ—è¡¨


{{< fluid_imgs
  "pure-u-1-1|https://static001.geekbang.org/resource/image/8e/67/8ee9f2fa987eccb490cfaa91c6484f67.png|Kubernetesé¡¹ç›®æ¶æ„|https://time.geekbang.org/column/article/23132"
>}}

- OCI: Open Container Initiative
- CNI: Container Network Interface
- CRI: Container Runtime Interface
- CSI: Container Storage Interface

ä½¿ç”¨ç¯å¢ƒWindowsã€‚

### WSL2 + Kubernetes

[åœ¨ Windows ä¸‹ä½¿ç”¨ WSL2 æ­å»º Kubernetes é›†ç¾¤](https://www.qikqiak.com/post/deploy-k8s-on-win-use-wsl2)

- å®‰è£…å•†åº—é‡Œçš„`windows terminal`
- æ›´æ–°ubuntuçš„è½¯ä»¶æº

```
cp /etc/apt/sources.list /etc/apt/sources.list.bak
root@k8s:~# echo "deb http://mirrors.aliyun.com/ubuntu/ focal main restricted
deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted
deb http://mirrors.aliyun.com/ubuntu/ focal universe
deb http://mirrors.aliyun.com/ubuntu/ focal-updates universe
deb http://mirrors.aliyun.com/ubuntu/ focal multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-updates multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted
deb http://mirrors.aliyun.com/ubuntu/ focal-security universe
deb http://mirrors.aliyun.com/ubuntu/ focal-security multiverse" > /etc/apt/sources.list
```

- kindå¯ä»¥ä»githubä¸Šä¸‹è½½åæ‰‹åŠ¨å®‰è£…â€”â€”æœ¬èº«æ˜¯goç¼–è¯‘çš„äºŒè¿›åˆ¶æ–‡ä»¶
- ä½†kindå®‰è£…kubernetesé›†ç¾¤æ—¶ä½¿ç”¨çš„æ˜¯ç¼–è¯‘å‘å¸ƒåœ¨hub.docker.comä¸Šçš„é•œåƒï¼ŒWSL2é‡Œé‡åˆ°ç½‘ç»œé—®é¢˜

>This will bootstrap a Kubernetes cluster using a pre-builtÂ [node image](https://kind.sigs.k8s.io/docs/design/node-image). Prebuilt images are hosted at[`kindest/node`](https://hub.docker.com/r/kindest/node/)

```
root@Gebitang:/home/geb# kind create cluster --name wslk8s
Creating cluster "wslk8s" ...
 âœ“ Ensuring node image (kindest/node:v1.21.1) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦
 âœ“ Writing configuration ğŸ“œ
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing CNI ğŸ”Œ
 âœ“ Installing StorageClass ğŸ’¾
Set kubectl context to "kind-wslk8s"
You can now use your cluster with:

kubectl cluster-info --context kind-wslk8s

Thanks for using kind! ğŸ˜Š
```

é‡æ–°ä½¿ç”¨`kubeadm`è¿›è¡Œå®‰è£…ï¼š

- éœ€è¦å…ˆå°†apt-key.gpgè¯ä¹¦å¯¼å…¥`curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -`ã€‚å¯ä»¥å…ˆå°†æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç„¶å`cat apt-get.gpg | apt-key add -`
- æ·»åŠ é•œåƒæºåˆ°åˆ—è¡¨ï¼šåœ¨`/etc/apt/sources.list.d/`ç›®å½•ä¸‹åˆ›å»º`kubernetes.list`æ–‡ä»¶ï¼Œå†…å®¹ä¸º`deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main`
- æ‰§è¡Œ`apt-get update && apt-get install -y kubeadm`

æ‰§è¡Œ`kubeadm version`è¿”å›ä¿¡æ¯ã€‚

### Errors were encountered while processing:  ubuntu-advantage-tools

ubuntu20.04æ‰§è¡Œ`apt-get upgrade`æ—¶é‡åˆ°ç±»ä¼¼ä¸Šé¢çš„é—®é¢˜ï¼Œçœ‹èµ·æ¥æ˜¯ä¸ª[bug](https://bugs.launchpad.net/ubuntu/+source/ubuntu-advantage-tools/+bug/1938097)ï¼Œå®˜æ–¹æç¤ºä¿®å¤æ–¹å¼ï¼š

- ç¼–è¾‘`/var/lib/dpkg/info/ubuntu-advantage-tools.postinst`
- æ‰§è¡Œ`dpkg --configure -a`
```
I think the problem is in the postinst script, line 295:

    cloud_id=""
    if command -v "cloud-id" > /dev/null ; then
      cloud_id=$(cloud-id)
    fi

The third line should rather be:

      cloud_id=$(cloud-id || true)
```


æ ¹æ®[äº‘åŸç”ŸæŠ€æœ¯åŸºç¡€](https://edu.aliyun.com/roadmap/cloudnative)çš„[ç¬¬ä¸‰è¯¾çš„demo](https://edu.aliyun.com/lesson_1651_16894)

### å‰æç¯å¢ƒ

ä¸‰ä»¶å¥—ï¼šVirturalBoxã€kubectlã€minikubeã€‚

- å®‰è£… VirtualBoxï¼šÂ [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads)

- å®‰è£…[kubectl on windows](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-windows)

ä¸‹è½½ã€æ·»åŠ PATHã€éªŒè¯ç‰ˆæœ¬

1.  Download the latest release v1.18.0 fromÂ [this link](https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/windows/amd64/kubectl.exe).

    Or if you haveÂ `curl`Â installed, use this command:

    ```
    curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/windows/amd64/kubectl.exe
    ```

    To find out the latest stable version (for example, for scripting), take a look atÂ [https://storage.googleapis.com/kubernetes-release/release/stable.txt](https://storage.googleapis.com/kubernetes-release/release/stable.txt).

2.  Add the binary in to your PATH.

3.  Test to ensure the version ofÂ `kubectl`Â is the same as downloaded:

    ```
    kubectl version --client
    ```


- å®‰è£…minikubeï¼Œ[åŸç”Ÿå®‰è£…](https://kubernetes.io/docs/tasks/tools/install-minikube/)ï¼Œ[aliyun: Minikube - Kubernetesæœ¬åœ°å®éªŒç¯å¢ƒ](https://developer.aliyun.com/article/221687)

å…ˆè¿›è¡Œç³»ç»Ÿæ£€æµ‹`systeminfo` 

```
Hyper-V Requirements:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.
Hyper-V è¦æ±‚:     å·²æ£€æµ‹åˆ°è™šæ‹Ÿæœºç›‘æ§ç¨‹åºã€‚å°†ä¸æ˜¾ç¤º Hyper-V æ‰€éœ€çš„åŠŸèƒ½ã€‚
```

---

å¯åŠ¨å‘½ä»¤ï¼š `minikube start --driver virtualbox`
```
D:\>minikube start --driver=virtualbox
* Microsoft Windows 10 Education 10.0.18363 Build 18363 ä¸Šçš„ minikube v1.12.3
* æ ¹æ®ç°æœ‰çš„é…ç½®æ–‡ä»¶ä½¿ç”¨ virtualbox é©±åŠ¨ç¨‹åº
* Starting control plane node minikube in cluster minikube
* Creating virtualbox VM (CPUs=4, Memory=6000MB, Disk=20000MB) ...
! StartHost failed, but will try again: creating host: create: precreate: This computer is running Hyper-V. VirtualBox won't boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check)
* Creating virtualbox VM (CPUs=4, Memory=6000MB, Disk=20000MB) ...
* Failed to start virtualbox VM. "minikube start" may fix it: creating host: create: precreate: This computer is running Hyper-V. VirtualBox won't boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check)
*
E0821 17:24:38.091674   11324 exit.go:76] &{ID:VBOX_HYPERV_64_BOOT Err:This computer is running Hyper-V. VirtualBox won't boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check)
precreate
k8s.io/minikube/pkg/minikube/machine.(*LocalClient).Create
        /app/pkg/minikube/machine/client.go:225
k8s.io/minikube/pkg/minikube/machine.timedCreateHost.func2
        /app/pkg/minikube/machine/start.go:188
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1373
create
k8s.io/minikube/pkg/minikube/machine.timedCreateHost
        /app/pkg/minikube/machine/start.go:197
k8s.io/minikube/pkg/minikube/machine.createHost
        /app/pkg/minikube/machine/start.go:163
k8s.io/minikube/pkg/minikube/machine.StartHost
        /app/pkg/minikube/machine/start.go:86
k8s.io/minikube/pkg/minikube/node.startHost
        /app/pkg/minikube/node/start.go:401
k8s.io/minikube/pkg/minikube/node.startMachine
        /app/pkg/minikube/node/start.go:345
k8s.io/minikube/pkg/minikube/node.Provision
        /app/pkg/minikube/node/start.go:234
k8s.io/minikube/cmd/minikube/cmd.provisionWithDriver
        /app/cmd/minikube/cmd/start.go:275
k8s.io/minikube/cmd/minikube/cmd.runStart
        /app/cmd/minikube/cmd/start.go:169
github.com/spf13/cobra.(*Command).execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:846
github.com/spf13/cobra.(*Command).ExecuteC
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
k8s.io/minikube/cmd/minikube/cmd.Execute
        /app/cmd/minikube/cmd/root.go:106
main.main
        /app/cmd/minikube/main.go:71
runtime.main
        /usr/local/go/src/runtime/proc.go:203
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1373
creating host
k8s.io/minikube/pkg/minikube/machine.createHost
        /app/pkg/minikube/machine/start.go:164
k8s.io/minikube/pkg/minikube/machine.StartHost
        /app/pkg/minikube/machine/start.go:86
k8s.io/minikube/pkg/minikube/node.startHost
        /app/pkg/minikube/node/start.go:401
k8s.io/minikube/pkg/minikube/node.startMachine
        /app/pkg/minikube/node/start.go:345
k8s.io/minikube/pkg/minikube/node.Provision
        /app/pkg/minikube/node/start.go:234
k8s.io/minikube/cmd/minikube/cmd.provisionWithDriver
        /app/cmd/minikube/cmd/start.go:275
k8s.io/minikube/cmd/minikube/cmd.runStart
        /app/cmd/minikube/cmd/start.go:169
github.com/spf13/cobra.(*Command).execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:846
github.com/spf13/cobra.(*Command).ExecuteC
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
k8s.io/minikube/cmd/minikube/cmd.Execute
        /app/cmd/minikube/cmd/root.go:106
main.main
        /app/cmd/minikube/main.go:71
runtime.main
        /usr/local/go/src/runtime/proc.go:203
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1373
Failed to start host
k8s.io/minikube/pkg/minikube/node.startMachine
        /app/pkg/minikube/node/start.go:347
k8s.io/minikube/pkg/minikube/node.Provision
        /app/pkg/minikube/node/start.go:234
k8s.io/minikube/cmd/minikube/cmd.provisionWithDriver
        /app/cmd/minikube/cmd/start.go:275
k8s.io/minikube/cmd/minikube/cmd.runStart
        /app/cmd/minikube/cmd/start.go:169
github.com/spf13/cobra.(*Command).execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:846
github.com/spf13/cobra.(*Command).ExecuteC
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
k8s.io/minikube/cmd/minikube/cmd.Execute
        /app/cmd/minikube/cmd/root.go:106
main.main
        /app/cmd/minikube/main.go:71
runtime.main
        /usr/local/go/src/runtime/proc.go:203
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1373 Advice:VirtualBox and Hyper-V are having a conflict. Use '--driver=hyperv' or disable Hyper-V using: 'bcdedit /set hypervisorlaunchtype off' URL: Issues:[4051 4783] ShowIssueLink:false}
* [VBOX_HYPERV_64_BOOT] error provisioning host Failed to start host: creating host: create: precreate: This computer is running Hyper-V. VirtualBox won't boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check)
* å»ºè®®ï¼šVirtualBox and Hyper-V are having a conflict. Use '--driver=hyperv' or disable Hyper-V using: 'bcdedit /set hypervisorlaunchtype off'
* ç›¸å…³é—®é¢˜ï¼š
  - https://github.com/kubernetes/minikube/issues/4051
  - https://github.com/kubernetes/minikube/issues/4783

```

æŠ¥é”™ä¿¡æ¯è¿˜æ˜¯å¾ˆæ¸…æ¥šï¼Œè¿è¡Œæ—¶å†²çªï¼š
- ä½¿ç”¨ç®¡ç†å‘˜æƒé™æ‰§è¡Œ`bcdedit /set hypervisorlaunchtype off`ï¼ŒæˆåŠŸåï¼Œè¿˜æ˜¯æç¤ºç›¸åŒçš„é—®é¢˜ã€‚
- ä½¿ç”¨`--virtualbox-no-vtx-check` æç¤ºè¿™ä¸ª`Error: unknown flag: --virtualbox-no-vtx-check` å¯ä»¥See `minikube start --help' for usage.`

æœç„¶ï¼Œç°åœ¨çš„å‚æ•°æ˜¯`--no-vtx-check=true`ã€‚æ‰€ä»¥å®Œæ•´çš„å‚æ•°åº”è¯¥æ˜¯`minikube start --driver=virtualbox --no-vtx-check=true`

ä¸å¹¸çš„æ—¶ï¼Œè¿˜æ˜¯é‡åˆ°é—®é¢˜äº†

```
PS D:\> minikube start --driver=virtualbox --no-vtx-check=true
* Microsoft Windows 10 Education 10.0.18363 Build 18363 ä¸Šçš„ minikube v1.12.3
* æ ¹æ®ç°æœ‰çš„é…ç½®æ–‡ä»¶ä½¿ç”¨ virtualbox é©±åŠ¨ç¨‹åº
* Starting control plane node minikube in cluster minikube
* Creating virtualbox VM (CPUs=4, Memory=6000MB, Disk=20000MB) ...
* æ­£åœ¨ Docker 19.03.12 ä¸­å‡†å¤‡ Kubernetes v1.18.3â€¦
* Unable to load cached images: loading cached images: transferring cached image: sudo test -d /var/lib/minikube/images && sudo scp -t /var/lib/minikube/images && sudo touch -d "2020-08-21 17:31:20.5418176 +0800" /var/lib/minikube/images/etcd_3.4.3-0: wait: remote command exited without exit status or exit signal
output:
...
stdout:

stderr:
fatal error: index out of range
runtime: panic before malloc heap initialized

```

æ‰§è¡Œ`bcdedit /set hypervisorlaunchtype off`ä¹‹åå‡ºç°dockeræ— æ³•å¯åŠ¨æƒ…å†µï¼Ÿç®¡ç†å‘˜æƒé™æ‰§è¡Œ`bcdedit /Set {current} hypervisorlaunchtype auto`æ¢å¤åˆå§‹å€¼ï¼Œæ‰§è¡Œ`bcdedit`å¯çœ‹åˆ°å…¨éƒ¨é…ç½®ä¿¡æ¯ã€‚

è‡ªåŠ¨æç¤ºé“¾æ¥ä¸º[VIRTUALIZATION MUST BE ENABLED](https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization-must-be-enabled)ï¼Œä½†æ£€æŸ¥æ˜¯ç¬¦åˆè¦æ±‚çš„ã€‚é‡å¯åå†çœ‹å§ã€‚æ¥ç€å…ˆå¾€ä¸‹å­¦è¯¾ç¨‹ã€‚


