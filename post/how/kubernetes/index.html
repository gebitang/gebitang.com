<!doctype html><html xmlns:wb=http://open.weibo.com/wb lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=Content-Security-Policy content="upgrade-insecure-requests"><meta name=description content="the way to kubernetes"><meta name=generator content="Hugo 0.128.0"><title>k8s practice &#183; 戈壁堂</title>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css><!--[if lte IE 8]><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css><![endif]--><!--[if gt IE 8]><!--><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css><!--<![endif]--><!--[if lte IE 8]><link rel=stylesheet href=http://gebitang.com/css/side-menu-old-ie.css><![endif]--><!--[if gt IE 8]><!--><link rel=stylesheet href=http://gebitang.com/css/side-menu.css><!--<![endif]--><link rel=stylesheet href=http://gebitang.com/css/blackburn.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.9.0/css/all.css><link href="https://fonts.googleapis.com/css?family=Raleway" rel=stylesheet type=text/css><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/androidstudio.min.css><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js></script><script>hljs.initHighlightingOnLoad()</script><link rel="shortcut icon" href=http://gebitang.com/img/favicon.ico type=image/x-icon><link rel=stylesheet href=http://gebitang.com/css/my.css><script src=http://gebitang.com/js/my.js></script></head><body><div id=layout><a href=#menu id=menuLink class=menu-link><span></span></a><div id=menu><a class="pure-menu-heading brand" href=http://gebitang.com/>Gebitang</a><div class=pure-menu><ul class=pure-menu-list><li class=pure-menu-item><a class=pure-menu-link href=http://gebitang.com/><i class='fa fa-home fa-fw'></i>Home</a></li><li class=pure-menu-item><a class=pure-menu-link href=http://gebitang.com/post/><i class='fa fa-list fa-fw'></i>Posts</a></li><li class=pure-menu-item><a class=pure-menu-link href=http://gebitang.com/about/><i class='fa fa-user fa-fw'></i>About</a></li><li class=pure-menu-item><a class=pure-menu-link href=http://gebitang.com/contact/><i class='fa fa-phone fa-fw'></i>Contact</a></li><li class=pure-menu-item><a class=pure-menu-link href=http://gebitang.com/tags/><i class='fa fa-tags fa-fw'></i>Tags</a></li><li class=pure-menu-item><a class=pure-menu-link href=http://gebitang.com/topics/><i class='fa fa-folder fa-fw'></i>Topics</a></li></ul></div><div class="pure-menu social"><ul class=pure-menu-list></ul></div><div><div class=small-print><small><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt=知识共享许可协议 style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/80x15.png></a><br>本<span xmlns:dct=http://purl.org/dc/terms/ href=http://purl.org/dc/dcmitype/Text rel=dct:type>作品</span>由<a xmlns:cc=http://creativecommons.org/ns# href=http://gebitang.com property="cc:attributionName" rel=cc:attributionURL>&nbsp;Gebitang&nbsp;&nbsp;</a>采用<a rel=license href=http://creativecommons.org/licenses/by/4.0/>知识共享署名 4.0 国际许可协议</a>进行许可&nbsp;&copy; 2018</small></div><div class=small-print><small></small></div><div class=small-print><small>Built with&nbsp;<a href=https://gohugo.io/ target=_blank>Hugo</a></small>
<small>Theme&nbsp;<a href=https://github.com/yoshiharuyamashita/blackburn target=_blank>Blackburn</a></small></div><div style=display:none><div class=small-print><small><wb:follow-button uid=2446212465 type=red_1 width=67 height=24></wb:follow-button></small></div></div></div></div><div id=main><div class=header><h1>k8s practice</h1><h2>the way to kubernetes</h2></div><div class=content><div class=post-meta><div><i class="fa fa-calendar fa-fw"></i>
<time>2022-03-25</time></div><div><i class="fa fa-folder fa-fw"></i>
<a class=post-taxonomy-topic href=http://gebitang.com/topics/kubernetes>kubernetes</a>&nbsp;/
<a class=post-taxonomy-topic href=http://gebitang.com/topics/k8s>k8s</a></div><div><i class="fa fa-tags fa-fw"></i>
<a class=post-taxonomy-tag href=http://gebitang.com/tags/kubernetes>kubernetes</a>&nbsp;/
<a class=post-taxonomy-tag href=http://gebitang.com/tags/k8s>k8s</a></div></div><nav id=TableOfContents><ul><li><a href=#nodeport类型服务>NodePort类型服务</a></li><li><a href=#k8s-恢复>k8s 恢复</a></li><li><a href=#k8s-搭建>k8s 搭建</a><ul><li><a href=#前置配置>前置配置</a><ul><li><a href=#dnf>dnf</a></li></ul></li><li><a href=#安装docker服务>安装docker服务</a></li><li><a href=#安装kubernetes服务>安装kubernetes服务</a></li><li><a href=#准备脚本>准备脚本</a></li><li><a href=#部署集群>部署集群</a></li><li><a href=#安装网络插件>安装网络插件</a></li><li><a href=#架构检查>架构检查</a><ul><li><a href=#pause容器>pause容器</a></li></ul></li><li><a href=#后置检查检查>后置检查检查</a></li><li><a href=#删除集群>删除集群</a></li></ul></li><li><a href=#k8s-practice>k8s practice</a><ul><li><a href=#获取log日志>获取log日志</a></li><li><a href=#调度管理>调度管理</a></li><li><a href=#断电重启问题>断电重启问题</a></li><li><a href=#创建configmap-扩容deploy>创建configMap 扩容deploy</a></li><li><a href=#troubleshooting-a-deployment>Troubleshooting a Deployment</a></li><li><a href=#证书管理>证书管理</a></li><li><a href=#安装lxcfs服务>安装lxcfs服务</a></li><li><a href=#强制删除pod>强制删除pod</a></li><li><a href=#deployment的字段含义>deployment的字段含义</a></li><li><a href=#拉取镜像卡住>拉取镜像卡住</a></li><li><a href=#问题排查流程>问题排查流程</a></li><li><a href=#kubectl-cp---help>kubectl cp &ndash;help</a></li><li><a href=#获取网关信息>获取网关信息：</a></li><li><a href=#exec-多容器>exec 多容器</a></li><li><a href=#deployment>deployment</a></li><li><a href=#display-all-kubernetes-objects>display all kubernetes objects</a></li><li><a href=#部署ingress-nginx>部署ingress-nginx</a><ul><li><a href=#1x-vs-0x-branch>1.x VS. 0.x Branch</a></li></ul></li><li><a href=#informers>informers</a></li></ul></li><li><a href=#tasks>tasks</a><ul><li><a href=#更新daemonset>更新DaemonSet</a></li><li><a href=#管理内存cpu配额>管理内存、CPU、配额</a></li></ul></li><li><a href=#start-over>start over</a><ul><li><a href=#虚拟机环境>虚拟机环境</a><ul><li><a href=#systemd-vs-cgroupfs>systemd vs cgroupfs</a></li></ul></li><li><a href=#纯手工搭建环境>纯手工搭建环境</a></li><li><a href=#wsl2-时间同步>wsl2 时间同步</a></li><li><a href=#kubernetes-tutorial>Kubernetes Tutorial</a></li><li><a href=#k8s中的crd开发>K8S中的CRD开发</a></li><li><a href=#setup-with-dashboard>setup with dashboard</a></li><li><a href=#wsldocker的限制>WSL+Docker的限制</a></li></ul></li><li><a href=#archive>archive</a><ul><li><a href=#wsl2--kubernetes>WSL2 + Kubernetes</a></li><li><a href=#errors-were-encountered-while-processing--ubuntu-advantage-tools>Errors were encountered while processing: ubuntu-advantage-tools</a></li><li><a href=#前提环境>前提环境</a></li></ul></li></ul></nav><div class=pure-g><div class=pure-u-1-1><div style="padding:0 .2em"><a href=https://time.geekbang.org/column/article/23132 target=_blank><img class=pure-img-responsive src=https://static001.geekbang.org/resource/image/8e/67/8ee9f2fa987eccb490cfaa91c6484f67.png alt title=Kubernetes项目架构></a></div></div></div><ul><li>OCI: Open Container Initiative</li><li>CNI: Container Network Interface</li><li>CRI: Container Runtime Interface</li><li>CSI: Container Storage Interface</li></ul><p><a href=https://mp.weixin.qq.com/s/NYhUQqaN9v1s36EHqpw-kA>OCI运行时规范</a></p><ul><li>符合 CRI 标准的 containerd，以及底层的 runC，都是从Docker 项目中分拆出来。<ul><li>2015 年OCI成立之初，Docker 的 libcontainer 项目被捐赠给OCI，成为独立的容器运行时项目 runC</li><li>2017年，Docker公司高层容器运行时的功能集中到containerd项目里，捐赠给云原生计算基金会。</li><li>Docker 引擎在发布时是一个单体应用，后来按功能分拆成 runC 和 containerd 两个不同层次的运行时。</li></ul></li><li>CRI-O是替代Docker或者containerd的高效且轻量级的容器运行时方案，是CRI的一个实现，能够运行符合OCI规范的容器，所以被称为CRI-O。</li></ul><div class=pure-g><div class=pure-u-1-1><div style="padding:0 .2em"><a href target=_blank><img class=pure-img-responsive src=https://s3-img.meituan.net/v1/mss_3d027b52ec5a4d589e68050845611e68/ff/n0/0n/34/dp_301168.jpg alt title=OCI></a></div></div></div><h2 id=nodeport类型服务>NodePort类型服务</h2><p>默认情况下，NodePort类型Service可以通过任意节点IP:节点端口访问。</p><h2 id=k8s-恢复>k8s 恢复</h2><ul><li><a href=https://blog.csdn.net/qq_34556414/article/details/113882631>ETCD 集群的备份和恢复</a></li><li><a href="http://www.xuyasong.com/?p=2052">K8S 集群备份与恢复</a></li></ul><pre tabindex=0><code># 执行备份
date;
CACERT=&#34;/opt/kubernetes/ssl/ca.pem&#34;
CERT=&#34;/opt/kubernetes/ssl/server.pem&#34;
EKY=&#34;/opt/kubernetes/ssl/server-key.pem&#34;
ENDPOINTS=&#34;127.0.0.1:2379&#34;

ETCDCTL_API=3 etcdctl \
--cacert=&#34;${CACERT}&#34; --cert=&#34;${CERT}&#34; --key=&#34;${EKY}&#34; \
--endpoints=${ENDPOINTS} \
snapshot save /backup/etcd-snapshot-`date +%Y%m%d`.db

# 备份保留30天
find /backup/ -name *.db -mtime +30 -exec rm -f {} \;

# 恢复
1.停止所有 Master 上 kube-apiserver 服务

systemctl stop kube-apiserver

2.停止集群中所有 ETCD 服务
systemctl stop etcd

3.移除所有 ETCD 存储目录下数据
mv /etcd/data /etcd/data.bak

4.从备份文件中恢复数据
ETCDCTL_API=3 etcdctl snapshot restore /backup/etcd-snapshot-xx.db  

5.启动 etcd
systemctl start etcd

6.启动 apiserver
systemctl start kube-apiserver

7.检查服务是否正常
</code></pre><h2 id=k8s-搭建>k8s 搭建</h2><p>在CentOS机器上安装，配置1个master+2个node机器</p><h3 id=前置配置>前置配置</h3><ul><li>关闭防火墙 （保证网络通畅）</li><li>关闭<code>swap</code>分区（防止kubelet重启失败）</li><li>设置主机名（master节点可识别到各个node名称）</li><li>添加hosts解析 （将集群所有的ip和对应的主机名添加到各自的hosts文件中国）</li><li>打开<code>ipv6</code>流量转发</li><li>配置国内镜像源</li><li>时间同步服务</li></ul><p>操作过程——</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># close firewall</span>
</span></span><span style=display:flex><span>systemctl disable --now firewalld
</span></span><span style=display:flex><span>setenforce <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>sed -i <span style=color:#e6db74>&#39;s/enforcing/disabled/&#39;</span> /etc/selinux/config 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># close swap </span>
</span></span><span style=display:flex><span><span style=color:#75715e># comment the line contain swap config in </span>
</span></span><span style=display:flex><span>swapoff -a
</span></span><span style=display:flex><span>sed -i.bak <span style=color:#e6db74>&#39;s/^.*centos-swap/#&amp;/g&#39;</span> /etc/fstab
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># set host name</span>
</span></span><span style=display:flex><span>hostnamectl set-hostname master
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add file /etc/sysctl.d/k8s.conf  with following content</span>
</span></span><span style=display:flex><span>net.ipv4.ip_forward <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>net.bridge.bridge-nf-call-ip6tables <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>net.bridge.bridge-nf-call-iptables <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#75715e># make it work at once</span>
</span></span><span style=display:flex><span>sysctl --system 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># config native yum repo.</span>
</span></span><span style=display:flex><span>mv /etc/yum.repos.d/* /tmp <span style=color:#75715e># back up first</span>
</span></span><span style=display:flex><span>curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
</span></span><span style=display:flex><span>curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># sync time</span>
</span></span><span style=display:flex><span>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
</span></span><span style=display:flex><span>yum install dnf ntpdate -y <span style=color:#75715e># install dnf </span>
</span></span><span style=display:flex><span>dnf makecache
</span></span><span style=display:flex><span>ntpdate ntp.aliyun.com
</span></span></code></pre></div><h4 id=dnf>dnf</h4><p>DNF(Dandified Yum)是新一代的RPM软件包管理器</p><pre tabindex=0><code># 列出所有 RPM 包
dnf list

# 安装软件包
dnf install wget

# 删除软件包
dnf remove wget

# 查看所有的软件包组
dnf grouplist

# 安装一个软件包组
dnf groupinstall ‘安全性工具’

# 查看系统中可用的 DNF 软件库
dnf repolist

# 查看系统中可用和不可用的所有的 DNF 软件库
dnf repolist all

# 列出所有安装了的 RPM 包
dnf list installed

# 列出所有可供安装的 RPM 包
dnf list available

# 搜索软件库中的 RPM 包
dnf search wget

# 查找某一文件的提供者
dnf provides /bin/bash

# 查看软件包详情
dnf info wget

# 删除无用孤立的软件包
dnf autoremove

# 删除缓存的无用软件包
dnf clean all

# 获取有关某条命令的使用帮助
dnf help clean

# 查看 DNF 命令的执行历史
dnf history

# 从特定的软件包库安装特定的软件
dnf -enablerepo=epel install nginx

# 重新安装特定软件包
dnf reinstall wget
</code></pre><h3 id=安装docker服务>安装docker服务</h3><ul><li>配置国内源</li><li>安装并设置开机启动</li><li>检查版本</li><li>配置docker的镜像仓库</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># add native repo</span>
</span></span><span style=display:flex><span>curl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># install by dnf </span>
</span></span><span style=display:flex><span>dnf install -y  docker-ce docker-ce-cli
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># config auto start</span>
</span></span><span style=display:flex><span>systemctl enable --now docker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#check version</span>
</span></span><span style=display:flex><span>docker --version 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add customed repo for docker. </span>
</span></span><span style=display:flex><span>cat &gt; /etc/docker/daemon.json <span style=color:#e6db74>&lt;&lt; EOF
</span></span></span><span style=display:flex><span><span style=color:#e6db74>{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;registry-mirrors&#34;: [&#34;https://f1bhsuge.mirror.aliyuncs.com&#34;]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl restart docker
</span></span></code></pre></div><h3 id=安装kubernetes服务>安装kubernetes服务</h3><ul><li>配置国内源</li><li>安装组件kubeadm、kubelet、kubectl</li><li>设置开始启动 <code>systemctl enable kubelet</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat &gt; /etc/yum.repos.d/kubernetes.repo <span style=color:#e6db74>&lt;&lt; EOF
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[kubernetes]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>name=Kubernetes
</span></span></span><span style=display:flex><span><span style=color:#e6db74>baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
</span></span></span><span style=display:flex><span><span style=color:#e6db74>enabled=1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>gpgcheck=0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>repo_gpgcheck=0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># show version</span>
</span></span><span style=display:flex><span>dnf list kubeadm --showduplicates
</span></span><span style=display:flex><span><span style=color:#75715e># packename+version name</span>
</span></span><span style=display:flex><span>dnf install -y kubelet-1.20.10 kubeadm-1.20.10 kubectl-1.20.10
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># auto start</span>
</span></span><span style=display:flex><span>systemctl enable kubelet
</span></span></code></pre></div><h3 id=准备脚本>准备脚本</h3><p>上述环境下，root用户执行如下脚步一次性完工——（除啦hosts文件未更新）</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> closeFireWall<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	systemctl disable --now firewalld
</span></span><span style=display:flex><span>	setenforce <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>	sed -i <span style=color:#e6db74>&#39;s/enforcing/disabled/&#39;</span> /etc/selinux/config
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> closeSwap<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	swapoff -a
</span></span><span style=display:flex><span>	<span style=color:#75715e># sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstab</span>
</span></span><span style=display:flex><span>	sed -ri <span style=color:#e6db74>&#39;s/.*swap.*/#&amp;/&#39;</span> /etc/fstab 
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> setHostName<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    hostnamectl set-hostname master
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> supportIpv6Traffic<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	echo <span style=color:#e6db74>&#39;net.ipv4.ip_forward = 1&#39;</span> &gt;&gt; /etc/sysctl.d/k8s.conf
</span></span><span style=display:flex><span>	echo <span style=color:#e6db74>&#39;net.bridge.bridge-nf-call-ip6tables = 1&#39;</span> &gt;&gt; /etc/sysctl.d/k8s.conf
</span></span><span style=display:flex><span>	echo <span style=color:#e6db74>&#39;net.bridge.bridge-nf-call-iptables = 1&#39;</span> &gt;&gt; /etc/sysctl.d/k8s.conf
</span></span><span style=display:flex><span>	sysctl --system
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> updateYumRepo<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	mkdir -p /home/sa/repo.bak
</span></span><span style=display:flex><span>	mv /etc/yum.repos.d/* /home/sa/repo.bak/
</span></span><span style=display:flex><span>	curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
</span></span><span style=display:flex><span>	curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> syncTime<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
</span></span><span style=display:flex><span>	yum install dnf ntpdate -y <span style=color:#75715e># install dnf</span>
</span></span><span style=display:flex><span>	dnf makecache
</span></span><span style=display:flex><span>	ntpdate ntp.aliyun.com
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> installDocker<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># add native repo</span>
</span></span><span style=display:flex><span>	curl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span>	<span style=color:#75715e># install by dnf</span>
</span></span><span style=display:flex><span>	dnf install -y  docker-ce docker-ce-cli
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># add customed repo for docker.</span>
</span></span><span style=display:flex><span>	mkdir -p /etc/docker
</span></span><span style=display:flex><span>	touch /etc/docker/daemon.json
</span></span><span style=display:flex><span>	cat &gt; /etc/docker/daemon.json <span style=color:#e6db74>&lt;&lt; EOF
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	  &#34;registry-mirrors&#34;: [&#34;https://f1bhsuge.mirror.aliyuncs.com&#34;]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># config auto start</span>
</span></span><span style=display:flex><span>	systemctl enable --now docker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>#check version</span>
</span></span><span style=display:flex><span>	docker --version
</span></span><span style=display:flex><span>	systemctl restart docker
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> installK8sComponent<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	cat &gt; /etc/yum.repos.d/kubernetes.repo <span style=color:#e6db74>&lt;&lt; EOF
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	[kubernetes]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	name=Kubernetes
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	enabled=1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	gpgcheck=0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	repo_gpgcheck=0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># show version</span>
</span></span><span style=display:flex><span>	dnf list kubeadm --showduplicates
</span></span><span style=display:flex><span>	<span style=color:#75715e># packename+version name</span>
</span></span><span style=display:flex><span>	dnf install -y kubelet-1.17.0 kubeadm-1.17.0 kubectl-1.17.0
</span></span><span style=display:flex><span>    systemctl enable kubelet
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> installLxcfs<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># insall lxcfs </span>
</span></span><span style=display:flex><span>	yum -y install fuse-devel fuse  lxc-templates
</span></span><span style=display:flex><span>	wget  --no-check-certificate  https://copr-be.cloud.fedoraproject.org/results/ganto/lxc3/epel-7-x86_64/01041891-lxcfs/lxcfs-3.1.2-0.2.el7.x86_64.rpm
</span></span><span style=display:flex><span>	rpm -ivh lxcfs-3.1.2-0.2.el7.x86_64.rpm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	systemctl enable lxcfs
</span></span><span style=display:flex><span>	systemctl start lxcfs
</span></span><span style=display:flex><span>	<span style=color:#75715e># 运⾏lxcfs命令，指定 /proc ⽬录所在位置：</span>
</span></span><span style=display:flex><span>	sudo mkdir -p /var/lib/lxcfs
</span></span><span style=display:flex><span>	sudo lxcfs /var/lib/lxcfs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># 需要确保lxcfs服务正常启动</span>
</span></span><span style=display:flex><span>	systemctl status lxcfs 
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>closeFireWall
</span></span><span style=display:flex><span>closeSwap
</span></span><span style=display:flex><span><span style=color:#75715e># setHostName</span>
</span></span><span style=display:flex><span>supportIpv6Traffic
</span></span><span style=display:flex><span>updateYumRepo
</span></span><span style=display:flex><span>syncTime
</span></span><span style=display:flex><span>installDocker
</span></span><span style=display:flex><span>installK8sComponent
</span></span><span style=display:flex><span>installLxcfs
</span></span></code></pre></div><h3 id=部署集群>部署集群</h3><ul><li>生成预处理文件</li><li>预拉取镜像</li><li>部署主节点</li><li>生成加入节点命令</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># pre-config </span>
</span></span><span style=display:flex><span>kubeadm config print init-defaults &gt; kubeadm-init.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># pull image first </span>
</span></span><span style=display:flex><span>kubeadm config images pull --config kubeadm-init.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># start buildup master</span>
</span></span><span style=display:flex><span>kubeadm init --config kubeadm-init.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># To print a join command for worker/slave node</span>
</span></span><span style=display:flex><span>kubeadm token create --print-join-command
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kubeadm.k8s.io/v1beta2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>bootstrapTokens</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>groups</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>system:bootstrappers:kubeadm:default-node-token</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>token</span>: <span style=color:#ae81ff>abcdef.0123456789abcdef</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ttl</span>: <span style=color:#ae81ff>24h0m0s</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>usages</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>signing</span>
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>authentication</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>InitConfiguration</span>
</span></span><span style=display:flex><span><span style=color:#f92672>localAPIEndpoint</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>advertiseAddress</span>: <span style=color:#ae81ff>192.168.50.200</span> <span style=color:#75715e>#address						     </span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>bindPort</span>:  <span style=color:#ae81ff>6443</span> <span style=color:#75715e>#port</span>
</span></span><span style=display:flex><span><span style=color:#f92672>nodeRegistration</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>criSocket</span>: <span style=color:#ae81ff>/var/run/dockershim.sock</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>master</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>taints</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>effect</span>: <span style=color:#ae81ff>NoSchedule</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>key</span>: <span style=color:#ae81ff>node-role.kubernetes.io/master</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiServer</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>timeoutForControlPlane</span>: <span style=color:#ae81ff>4m0s</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kubeadm.k8s.io/v1beta2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>certificatesDir</span>: <span style=color:#ae81ff>/etc/kubernetes/pki</span>
</span></span><span style=display:flex><span><span style=color:#f92672>clusterName</span>: <span style=color:#ae81ff>kubernetes</span>
</span></span><span style=display:flex><span><span style=color:#f92672>controllerManager</span>: {}
</span></span><span style=display:flex><span><span style=color:#f92672>dns</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>CoreDNS</span>
</span></span><span style=display:flex><span><span style=color:#f92672>etcd</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>local</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>dataDir</span>: <span style=color:#ae81ff>/var/lib/etcd</span>
</span></span><span style=display:flex><span><span style=color:#f92672>imageRepository</span>: <span style=color:#ae81ff>registry.cn-hangzhou.aliyuncs.com/google_containers  </span> <span style=color:#75715e>#阿里云的镜像站点</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterConfiguration</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kubernetesVersion</span>: <span style=color:#ae81ff>v1.20.10			#kubernetes版本号</span>
</span></span><span style=display:flex><span><span style=color:#f92672>networking</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>dnsDomain</span>: <span style=color:#ae81ff>cluster.local	</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>serviceSubnet</span>: <span style=color:#ae81ff>192.168.99.0</span><span style=color:#ae81ff>/24		#选择默认即可，当然也可以自定义CIDR</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>podSubnet</span>: <span style=color:#ae81ff>192.168.98.0</span><span style=color:#ae81ff>/24			#添加pod网段</span>
</span></span><span style=display:flex><span><span style=color:#f92672>scheduler</span>: {}
</span></span></code></pre></div><p>部署完成后，必须有对应的网络插件才能让node直接正常通信，否则一直处于notReady状态，提示——</p><blockquote><p>KubeletNotReady<br>runtime network not ready: NetworkReady=false
reason:NetworkPluginNotReady
message:docker: network plugin is not ready: cni config uninitialized</p></blockquote><h3 id=安装网络插件>安装网络插件</h3><p>没有网络插件的集群基本不可用，通常使用calico，测试集群node小于50可直接使用<a href=https://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises>官方手册</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># download</span>
</span></span><span style=display:flex><span>curl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O
</span></span><span style=display:flex><span><span style=color:#75715e># apply</span>
</span></span><span style=display:flex><span>kubectl apply -f calico.yaml
</span></span></code></pre></div><h3 id=架构检查>架构检查</h3><div class=pure-g><div class=pure-u-1-1><div style="padding:0 .2em"><a href target=_blank><img class=pure-img-responsive src=https://s3-img.meituan.net/v1/mss_3d027b52ec5a4d589e68050845611e68/ff/n0/0n/5f/sa_507404.jpg alt title=kubernetes></a></div></div></div><p>安装完成后，集群中启动的容器大概包括以下内容——除了网络相关的内容，可以看到架构图上的内容都有对应的容器</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># docker ps = docker container ls</span>
</span></span><span style=display:flex><span>CONTAINER ID   IMAGE           COMMAND                  CREATED       STATUS       PORTS     NAMES
</span></span><span style=display:flex><span>7681553c1375   0cae8d5cc64c    <span style=color:#e6db74>&#34;kube-apiserver --ad…&#34;</span>   <span style=color:#ae81ff>2</span> hours ago   Up <span style=color:#ae81ff>2</span> hours             k8s_kube-apiserver_kube-apiserver-master_kube-system_3595fcec3afd46524fa623e9083e8e2b_88
</span></span><span style=display:flex><span>1f6af2af6504   303ce5db0e90    <span style=color:#e6db74>&#34;etcd --advertise-cl…&#34;</span>   <span style=color:#ae81ff>2</span> hours ago   Up <span style=color:#ae81ff>2</span> hours             k8s_etcd_etcd-master_kube-system_3c0c04a4c2e1acb0e625dcf83a547310_108
</span></span><span style=display:flex><span>e6250d39a65c   78c190f736b1    <span style=color:#e6db74>&#34;kube-scheduler --au…&#34;</span>   <span style=color:#ae81ff>3</span> hours ago   Up <span style=color:#ae81ff>3</span> hours             k8s_kube-scheduler_kube-scheduler-master_kube-system_75516e998e1ab97384d969d8ccd139db_55
</span></span><span style=display:flex><span>95fb0ecc7f06   5eb3b7486872    <span style=color:#e6db74>&#34;kube-controller-man…&#34;</span>   <span style=color:#ae81ff>3</span> hours ago   Up <span style=color:#ae81ff>3</span> hours             k8s_kube-controller-manager_kube-controller-manager-master_kube-system_ab2c4278e88f8bade2b4f3742e6fba77_57
</span></span><span style=display:flex><span>18c4a40b1c4f   pause:3.1       <span style=color:#e6db74>&#34;/pause&#34;</span>                 <span style=color:#ae81ff>3</span> hours ago   Up <span style=color:#ae81ff>3</span> hours             k8s_POD_kube-scheduler-master_kube-system_75516e998e1ab97384d969d8ccd139db_3
</span></span><span style=display:flex><span>b7b9eb44f106   pause:3.1       <span style=color:#e6db74>&#34;/pause&#34;</span>                 <span style=color:#ae81ff>3</span> hours ago   Up <span style=color:#ae81ff>3</span> hours             k8s_POD_kube-controller-manager-master_kube-system_ab2c4278e88f8bade2b4f3742e6fba77_3
</span></span><span style=display:flex><span>1e5402dc33eb   pause:3.1       <span style=color:#e6db74>&#34;/pause&#34;</span>                 <span style=color:#ae81ff>3</span> hours ago   Up <span style=color:#ae81ff>3</span> hours             k8s_POD_kube-apiserver-master_kube-system_3595fcec3afd46524fa623e9083e8e2b_3
</span></span><span style=display:flex><span>0ae0fee45898   pause:3.1       <span style=color:#e6db74>&#34;/pause&#34;</span>                 <span style=color:#ae81ff>3</span> hours ago   Up <span style=color:#ae81ff>3</span> hours             k8s_POD_etcd-master_kube-system_3c0c04a4c2e1acb0e625dcf83a547310_3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># kc get pods --all-namespaces</span>
</span></span><span style=display:flex><span>NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>kube-system   etcd-master                      1/1     Running   <span style=color:#ae81ff>108</span>        4h1m
</span></span><span style=display:flex><span>kube-system   kube-apiserver-master            1/1     Running   <span style=color:#ae81ff>88</span>         4h
</span></span><span style=display:flex><span>kube-system   kube-controller-manager-master   1/1     Running   <span style=color:#ae81ff>57</span>         4h1m
</span></span><span style=display:flex><span>kube-system   kube-scheduler-master            1/1     Running   <span style=color:#ae81ff>55</span>         4h1m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># docker images </span>
</span></span><span style=display:flex><span>kube-proxy                   v1.17.0   7d54289267dc   <span style=color:#ae81ff>2</span> years ago    116MB
</span></span><span style=display:flex><span>kube-scheduler               v1.17.0   78c190f736b1   <span style=color:#ae81ff>2</span> years ago    94.4MB
</span></span><span style=display:flex><span>kube-apiserver               v1.17.0   0cae8d5cc64c   <span style=color:#ae81ff>2</span> years ago    171MB
</span></span><span style=display:flex><span>kube-controller-manager      v1.17.0   5eb3b7486872   <span style=color:#ae81ff>2</span> years ago    161MB
</span></span><span style=display:flex><span>coredns                      1.6.5     70f311871ae1   <span style=color:#ae81ff>2</span> years ago    41.6MB
</span></span><span style=display:flex><span>etcd                         3.4.3-0   303ce5db0e90   <span style=color:#ae81ff>2</span> years ago    288MB
</span></span><span style=display:flex><span>pause                        3.1       da86e6ba6ca1   <span style=color:#ae81ff>4</span> years ago    742kB
</span></span><span style=display:flex><span>calico/kube-controllers      v3.22.1   c0c6672a66a5   <span style=color:#ae81ff>2</span> months ago   132MB
</span></span><span style=display:flex><span>calico/cni                   v3.22.1   2a8ef6985a3e   <span style=color:#ae81ff>2</span> months ago   236MB
</span></span><span style=display:flex><span>calico/pod2daemon-flexvol    v3.22.1   17300d20daf9   <span style=color:#ae81ff>2</span> months ago   19.7MB
</span></span><span style=display:flex><span>calico/node                  v3.22.1   7a71aca7b60f   <span style=color:#ae81ff>2</span> months ago   198MB
</span></span></code></pre></div><p>容器插件还没有安装，此时原生自带的资源包括——</p><pre tabindex=0><code>NAME                              SHORTNAMES   APIGROUP                       NAMESPACED   KIND
bindings                                                                      true         Binding
componentstatuses                 cs                                          false        ComponentStatus
configmaps                        cm                                          true         ConfigMap
endpoints                         ep                                          true         Endpoints
events                            ev                                          true         Event
limitranges                       limits                                      true         LimitRange
namespaces                        ns                                          false        Namespace
nodes                             no                                          false        Node
persistentvolumeclaims            pvc                                         true         PersistentVolumeClaim
persistentvolumes                 pv                                          false        PersistentVolume
pods                              po                                          true         Pod
podtemplates                                                                  true         PodTemplate
replicationcontrollers            rc                                          true         ReplicationController
resourcequotas                    quota                                       true         ResourceQuota
secrets                                                                       true         Secret
serviceaccounts                   sa                                          true         ServiceAccount
services                          svc                                         true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io           false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io         false        APIService
controllerrevisions                            apps                           true         ControllerRevision
daemonsets                        ds           apps                           true         DaemonSet
deployments                       deploy       apps                           true         Deployment
replicasets                       rs           apps                           true         ReplicaSet
statefulsets                      sts          apps                           true         StatefulSet
tokenreviews                                   authentication.k8s.io          false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io           true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io           false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io           false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io           false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling                    true         HorizontalPodAutoscaler
cronjobs                          cj           batch                          true         CronJob
jobs                                           batch                          true         Job
certificatesigningrequests        csr          certificates.k8s.io            false        CertificateSigningRequest
leases                                         coordination.k8s.io            true         Lease
endpointslices                                 discovery.k8s.io               true         EndpointSlice
events                            ev           events.k8s.io                  true         Event
ingresses                         ing          extensions                     true         Ingress
ingresses                         ing          networking.k8s.io              true         Ingress
networkpolicies                   netpol       networking.k8s.io              true         NetworkPolicy
runtimeclasses                                 node.k8s.io                    false        RuntimeClass
poddisruptionbudgets              pdb          policy                         true         PodDisruptionBudget
podsecuritypolicies               psp          policy                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io      false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io      false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io      true         RoleBinding
roles                                          rbac.authorization.k8s.io      true         Role
priorityclasses                   pc           scheduling.k8s.io              false        PriorityClass
csidrivers                                     storage.k8s.io                 false        CSIDriver
csinodes                                       storage.k8s.io                 false        CSINode
storageclasses                    sc           storage.k8s.io                 false        StorageClass
volumeattachments                              storage.k8s.io                 false        VolumeAttachment
</code></pre><h4 id=pause容器>pause容器</h4><ul><li>镜像非常小，目前在 700KB 左右</li><li>永远处于 Pause (暂停) 状态</li><li>在 pod 中担任 Linux 命名空间共享的基础；</li><li>启用 pid 命名空间，开启 init 进程。</li></ul><p>完成pod里的多个容器的网络共享：使用统一个网络、同一个ip</p><p>参考<a href=https://jimmysong.io/kubernetes-handbook/concepts/pause-container.html>Pause 容器</a>或<a href=https://www.ianlewis.org/en/almighty-pause-container>The Almighty Pause Container</a></p><h3 id=后置检查检查>后置检查检查</h3><p>确保对应点docker、kubelet服务正常启动，防火墙关闭，swap分区关闭(防止kubelet启动失败)、端口没有占用。查看对应服务日志，例如<code>journalctl -xeu kubelet</code></p><h3 id=删除集群>删除集群</h3><p>在所有节点上执行以下动作：</p><ul><li>重置集群</li><li>删除对应配置</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># reset </span>
</span></span><span style=display:flex><span>kubeadm reset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># delete related files</span>
</span></span><span style=display:flex><span>rm -rf /var/lib/kubelet
</span></span><span style=display:flex><span>rm -rf /etc/kubernetes
</span></span><span style=display:flex><span>rm -rf  /etc/cni/net.d
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#f92672>[</span>root@master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubeadm reset</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> Reading configuration from the cluster...
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> FYI: You can look at this config file with <span style=color:#e6db74>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
</span></span><span style=display:flex><span>W0517 17:57:47.281074   <span style=color:#ae81ff>10646</span> reset.go:99<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: configmaps <span style=color:#e6db74>&#34;kubeadm-config&#34;</span> not found
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> WARNING: Changes made to this host by <span style=color:#e6db74>&#39;kubeadm init&#39;</span> or <span style=color:#e6db74>&#39;kubeadm join&#39;</span> will be reverted.
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> Are you sure you want to proceed? <span style=color:#f92672>[</span>y/N<span style=color:#f92672>]</span>: y
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>preflight<span style=color:#f92672>]</span> Running pre-flight checks
</span></span><span style=display:flex><span>W0517 17:58:00.257625   <span style=color:#ae81ff>10646</span> removeetcdmember.go:79<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> No kubeadm config, using etcd pod spec to get data directory
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> Stopping the kubelet service
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> Unmounting mounted directories in <span style=color:#e6db74>&#34;/var/lib/kubelet&#34;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> Deleting contents of config directories: <span style=color:#f92672>[</span>/etc/kubernetes/manifests /etc/kubernetes/pki<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> Deleting files: <span style=color:#f92672>[</span>/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>reset<span style=color:#f92672>]</span> Deleting contents of stateful directories: <span style=color:#f92672>[</span>/var/lib/etcd /var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The reset process does not clean CNI configuration. To <span style=color:#66d9ef>do</span> so, you must remove /etc/cni/net.d
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The reset process does not reset or clean up iptables rules or IPVS tables.
</span></span><span style=display:flex><span>If you wish to reset iptables, you must <span style=color:#66d9ef>do</span> so manually by using the <span style=color:#e6db74>&#34;iptables&#34;</span> command.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>If your cluster was setup to utilize IPVS, run ipvsadm --clear <span style=color:#f92672>(</span>or similar<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>to reset your system<span style=color:#960050;background-color:#1e0010>&#39;</span>s IPVS tables.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The reset process does not clean your kubeconfig files and you must remove them manually.
</span></span><span style=display:flex><span>Please, check the contents of the $HOME/.kube/config file.
</span></span></code></pre></div><h2 id=k8s-practice>k8s practice</h2><h3 id=获取log日志>获取log日志</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run the command and capture the output</span>
</span></span><span style=display:flex><span>out<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;</span><span style=color:#66d9ef>$(</span>kubectl get pods<span style=color:#66d9ef>)</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Filter the output line by line using grep</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> read -r line; <span style=color:#66d9ef>do</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># -q option is used to suppress the output of grep</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> echo <span style=color:#e6db74>&#34;</span>$line<span style=color:#e6db74>&#34;</span> | grep -q <span style=color:#e6db74>&#34;pattern&#34;</span>; <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># -F&#39; &#39; option specifies the field separator as -</span>
</span></span><span style=display:flex><span>        echo <span style=color:#e6db74>&#34;</span>$line<span style=color:#e6db74>&#34;</span> | awk -F<span style=color:#e6db74>&#39; &#39;</span> <span style=color:#e6db74>&#39;print {&#34;kc logs -f --singce=1h  &#34; $1  &#34; | grep combineTask&#34;}&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>fi</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span> <span style=color:#f92672>&lt;&lt;&lt;</span> <span style=color:#e6db74>&#34;</span>$out<span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><h3 id=调度管理>调度管理</h3><p>cordon，drain，delete</p><p>cordon</p><blockquote><p>影响最小，只会将 node 调为 SchedulingDisabled，之后再发创建 pod，不会被调度到该节点，旧有的pod不会受到影响，仍正常对外提供服务</p></blockquote><pre tabindex=0><code>kubectl cordon &lt;node name&gt;
kubectl uncordon &lt;node name&gt;
</code></pre><p>drain</p><blockquote><p>驱逐node上的pod，其他节点重新创建；接着，将节点调为 SchedulingDisabled</p></blockquote><p>delete</p><blockquote><p>驱逐node上的pod，其他节点重新创建； 从master节点删除该node，master对其不可见，失去对其控制，master不可对其恢复<br>恢复调度，需进入node节点，重启kubelet， 基于node的自注册功能，节点重新恢复使用</p></blockquote><p><code>systemctl restart kubelet</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># 确定要排空的节点的名称</span>
</span></span><span style=display:flex><span>kubectl get nodes 
</span></span><span style=display:flex><span><span style=color:#75715e># 查看获取pod名字</span>
</span></span><span style=display:flex><span>kubectl get po 
</span></span><span style=display:flex><span><span style=color:#75715e># 命令node节点开始释放所有pod，并且不接收新的pod进程</span>
</span></span><span style=display:flex><span>kubectl drain <span style=color:#f92672>[</span>node-name<span style=color:#f92672>]</span> --force --ignore-daemonsets --delete-local-data 
</span></span><span style=display:flex><span><span style=color:#75715e># 这时候把需要做的事情做一下。比如上面说的更改docker文件daemon.json或者说node节点故障需要进行的处理操作 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># 然后恢复node，恢复接收新的pod进程</span>
</span></span><span style=display:flex><span>kubectl uncordon <span style=color:#f92672>[</span>node-name<span style=color:#f92672>]</span>
</span></span></code></pre></div><h3 id=断电重启问题>断电重启问题</h3><p>表明上提示<code>The connection to the server master:6443 was refused - did you specify the right host or port?</code>，最终实际上是因为apiserver无法连接etcd导致容器退出。</p><h3 id=创建configmap-扩容deploy>创建configMap 扩容deploy</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># kc create configmap --help查看</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 生成configmap文件</span>
</span></span><span style=display:flex><span>kubectl create configmap --dry-run<span style=color:#f92672>=</span>true saber.yml --from-file<span style=color:#f92672>=</span>./saber.yml --output yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl create configmap --dry-run<span style=color:#f92672>=</span>true pool.conf --from-file<span style=color:#f92672>=</span>./nginx.conf --output yaml &gt; bridgeconfig.yml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#扩容</span>
</span></span><span style=display:flex><span>kc scale deploy  nginx -n nginx-deploy --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 编辑部署文件</span>
</span></span><span style=display:flex><span>kc edit deploy nginx-deploy -n nginx-deploy
</span></span></code></pre></div><h3 id=troubleshooting-a-deployment>Troubleshooting a Deployment</h3><p>部署deployment后，pod一直未创建。处理流程——</p><ul><li><code>kc get deploy -n nspace</code>查看部署信息</li><li><code>kc describe deploy deploy-name</code>查看描述信息，只显示<code> Normal ScalingReplicaSet 8m48s deployment-controller Scaled up replica set oops-ssh-server-sshd-74cd68dd8 to 1</code></li><li><code>kc get replicaset -n nspace</code>查看对应的replicaset信息</li><li><code>kc describe replicaset name-of-replicaset</code>可以从events中看到对应的提示信息，例如<code>serviceaccount "oops-ssh-server" not found</code></li></ul><h3 id=证书管理>证书管理</h3><p>通过 kubeadm 安装的 Kubernetes，所有证书都存放在 <code>/etc/kubernetes/pki</code> 目录下</p><ul><li><a href=https://kubernetes.io/zh/docs/tasks/tls/manual-rotation-of-ca-certificates/>手动轮换 CA 证书</a></li><li><a href=https://kubernetes.io/zh/docs/setup/best-practices/certificates>PKI 证书和要求</a></li></ul><h3 id=安装lxcfs服务>安装lxcfs服务</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># 安装</span>
</span></span><span style=display:flex><span>yum -y install fuse-devel fuse  lxc-templates
</span></span><span style=display:flex><span>wget  --no-check-certificate  https://copr-be.cloud.fedoraproject.org/results/ganto/lxc3/epel-7-x86_64/01041891-lxcfs/lxcfs-3.1.2-0.2.el7.x86_64.rpm
</span></span><span style=display:flex><span>rpm -ivh lxcfs-3.1.2-0.2.el7.x86_64.rpm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl enable lxcfs
</span></span><span style=display:flex><span>systemctl start lxcfs
</span></span><span style=display:flex><span><span style=color:#75715e># 运⾏lxcfs命令，指定 /proc ⽬录所在位置：</span>
</span></span><span style=display:flex><span>mkdir -p /var/lib/lxcfs
</span></span><span style=display:flex><span>lxcfs /var/lib/lxcfs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 需要确保lxcfs服务正常启动</span>
</span></span><span style=display:flex><span>systemctl status lxcfs 
</span></span></code></pre></div><h3 id=强制删除pod>强制删除pod</h3><p><a href=https://stackoverflow.com/questions/35453792/pods-stuck-in-terminating-status>Pods stuck in Terminating status</a>，状态一直处于Terminating，强制删除</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#删除单个pod</span>
</span></span><span style=display:flex><span>kubectl delete pod <span style=color:#f92672>{</span>podname<span style=color:#f92672>}</span> -n <span style=color:#f92672>{</span>namespace<span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 配合必要的 ns参数 -n kube-system</span>
</span></span><span style=display:flex><span>ns<span style=color:#f92672>=</span>kube-system
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> p in <span style=color:#66d9ef>$(</span>kubectl get pods -n $ns | grep Terminating | awk <span style=color:#e6db74>&#39;{print $1}&#39;</span><span style=color:#66d9ef>)</span>; <span style=color:#66d9ef>do</span> kubectl delete pod $p -n $ns --grace-period<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> --force;<span style=color:#66d9ef>done</span>
</span></span></code></pre></div><p>提示告警：</p><blockquote><p>warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.</p></blockquote><h3 id=deployment的字段含义>deployment的字段含义</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-nginx</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app: my-nginx # LABEL-A</span>: <span style=color:#ae81ff>&lt;--this label is to manage the deployment itself. this may be used to filter the deployment based on this label. </span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:              
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app: my-nginx    #LABEL-B</span>:  <span style=color:#ae81ff>&lt;--  field defines how the Deployment finds which Pods to manage.</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app: my-nginx   #LABEL-C</span>: <span style=color:#ae81ff>&lt;--this is the label of the pod, this must be same as LABEL-B</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:                
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>hostAliases</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>ip</span>: <span style=color:#e6db74>&#34;192.168.0.10&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>hostnames</span>:
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;alias1.example.com&#34;</span>
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;alias2.example.com&#34;</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>ip</span>: <span style=color:#e6db74>&#34;192.168.0.20&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>hostnames</span>:
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;alias3.example.com&#34;</span>
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;alias4.example.com&#34;</span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-nginx</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx:alpine</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>limits</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;128Mi&#34;</span> <span style=color:#75715e>#128 MB</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;200m&#34;</span> <span style=color:#75715e>#200 millicpu (.2 cpu or 20% of the cpu)</span>
</span></span></code></pre></div><p>从运行中的k8s中获取deployment</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># show all in default ns</span>
</span></span><span style=display:flex><span>kubectl get deployments 
</span></span><span style=display:flex><span><span style=color:#75715e># extract one</span>
</span></span><span style=display:flex><span>kubectl get deployment deploy-name -o yaml &gt; name.yaml
</span></span></code></pre></div><h3 id=拉取镜像卡住>拉取镜像卡住</h3><p><a href=https://github.com/kubernetes/kubernetes/issues/83471>Pods get stuck in ContainerCreating state when pulling image takes long #83471</a> 看起来都遇到过这个问题。现象为kubelet一直处于拉取镜像状态。</p><ul><li>手动在对应的node节点上使用docker pull拉取对应的镜像OK</li><li>在对应的node上检查kubelet对应时段的log信息<code>journalctl -u kubelet --since "2022-04-12 14:00" --until "2022-04-12 14:20"</code></li><li>检查对应的镜像是否过大</li><li>kubelet开启并行下载，同时部署其他的镜像是否也会出现卡住现象？<code>--serialize-image-pulls=false</code></li></ul><h3 id=问题排查流程>问题排查流程</h3><ul><li><p>获取非running状态的pod<br><code>kc get pods -n qa-test |grep -v Running</code></p></li><li><p>查看问题pod状态：查看对应事件<br><code>kc describe pod pod-name -n group-name</code></p></li><li><p>查看pod的log信息<br><code>kc logs pod-name -n group-name</code></p></li><li><p>删除问题pod<br><code>kc delete pod pod-name -n group-name</code></p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kc get pod -n medusa |grep oops
</span></span><span style=display:flex><span>kc get daemonset -n medusa-system
</span></span><span style=display:flex><span>kc get daemonset secrets-daemon -n medusa-system -o yaml
</span></span><span style=display:flex><span>kc get daemonset -n medusa-system -o wide
</span></span><span style=display:flex><span>kc get pods -n medusa-system -o wide | grep secrets-daemon
</span></span><span style=display:flex><span>kc get daemonset secrets-daemon -n medusa-system -o yaml 
</span></span><span style=display:flex><span>kc exec -it secrets-daemon-zrg5w -n medusa-system /bin/bash
</span></span><span style=display:flex><span>kc get pod secrets-daemon-zrg5w -n medusa-system -o yaml
</span></span><span style=display:flex><span>kc describe pod secrets-daemon-zrg5w -n medusa-system
</span></span><span style=display:flex><span>kc logs secrets-daemon-zrg5w -n medusa-system  --tail<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>history <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><h3 id=kubectl-cp---help>kubectl cp &ndash;help</h3><p>Copy files and directories to and from containers.</p><h3 id=获取网关信息>获取网关信息：</h3><p>kc get pod -n istio-system -o wide | grep frontier
kc get pod -n medusa-system -o wide | grep nginx</p><h3 id=exec-多容器>exec 多容器</h3><p>多个容器可自定义名称，然后指定时使用<code>-c container-name</code>即可。例如<code>kc exec -it pod-name -c container-name -- ls /etc/</code></p><h3 id=deployment>deployment</h3><p>平台上的实例名对应集群中的 deployment的name；</p><p><code>kc describe deployment unit-test-platform -n qa-test</code></p><h3 id=display-all-kubernetes-objects>display all kubernetes objects</h3><ul><li>列出所有对象 <code>kubectl api-resources -o wide</code></li><li>查看对象信息 <code>kc explain name-of-object</code></li><li>常用手册查询： <a href=https://kubernetes.io/docs/reference/kubectl/cheatsheet/>kubectl cheat sheet</a></li><li><a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md>k8s API Conventions</a></li><li><a href=https://kubernetes.io/zh/docs/concepts/services-networking/ingress/>ingress</a></li></ul><p>获取 <code>api-resources</code>时，提示<code>error: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request</code>，相当于对于<code>metrics.k8s.io/v1beta1</code>这个apigroup无法返回对应的类型。其中包含<code>nodes</code>和<code>pods</code></p><pre tabindex=0><code>NAME                              SHORTNAMES     APIGROUP                       NAMESPACED   KIND
nodes                                            metrics.k8s.io                 false        NodeMetrics
pods                                             metrics.k8s.io                 true         PodMetrics
</code></pre><h3 id=部署ingress-nginx>部署ingress-nginx</h3><ul><li><a href=https://kubernetes.github.io/ingress-nginx/developer-guide/code-overview/>工作原理官方介绍</a></li><li><a href=https://kubernetes.github.io/ingress-nginx/how-it-works/>工作原理</a>，这里提到的 “同步循环模式”(synchronization loop pattern)，对应文档为 <a href=https://github.com/coreos/docs/blob/master/kubernetes/replication-controller.md>replication-controller</a></li><li><a href=https://zou.cool/2021/11/24/ingress-nginx-read/>Nginx-Ingress-Controller启动流程</a></li><li><a href=https://segmentfault.com/a/1190000017563487>如何编写正确且高效的 OpenResty 应用</a>，配合代码理解nginx.conf的内容执行逻辑</li><li><a href=https://moonbingbing.gitbooks.io/openresty-best-practices/content/lua/FFI.html>OpenResty最佳实践</a></li></ul><h4 id=1x-vs-0x-branch>1.x VS. 0.x Branch</h4><p>This is a breaking change between previous v0.X branch</p><p>This release only supports Kubernetes versions >= v1.19. The support for Ingress Object in networking.k8s.io/v1beta is being dropped and manifests should now use networking.k8s.io/v1.</p><pre tabindex=0><code>
# online:
NGINX Ingress controller
  Release:       v0.42.0
  Build:         e98e48d99abd6e65b761a66ed3a6a093f1ed16ec
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.19.6

Prepare for release v0.42.0 Manuel Alejandro de Brito Fontes 2020/12/24 22:47
e98e48d99abd6e65b761a66ed3a6a093f1ed16ec

=======================

# dev 
NGINX Ingress controller
  Release:       v0.49.3
  Build:         7ee28f431ccddc4997ab088eff2698b94a8918a0
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.19.9

Prepare for v0.49.3 release (#7742) Ricardo Katz* 2021/10/4 9:37
7ee28f431ccddc4997ab088eff2698b94a8918a0
</code></pre><h3 id=informers>informers</h3><ul><li><a href=https://github.com/kubernetes/client-go>client-go</a>源码informer文件夹下对应的实现</li><li><a href=https://www.kubernetes.org.cn/2693.html>Kubernetes Informer 详解</a>，<a href=https://zhuanlan.zhihu.com/p/59660536>informer详解</a>——这两篇类似，可以看做入门。说明了整体的实现逻辑。做法使用方式的了解，完全足够。</li><li><a href=https://www.cnblogs.com/tencent-cloud-native/p/15268959.html>如何高效掌控K8s资源变化？K8s Informer实现机制浅析</a>最新代码解析</li><li><a href=https://jimmysong.io/kubernetes-handbook/develop/client-go-informer-sourcecode-analyse.html>informer源码分析</a>，未读，可参考</li></ul><h2 id=tasks>tasks</h2><p>练习记录，<a href=https://kubernetes.io/docs/tasks/>官方Tasks</a>，<a href=https://kubernetes.io/zh/docs/tasks/>中文版本对照</a>跟着练习一遍之后，各个概念就清楚了。</p><h3 id=更新daemonset>更新DaemonSet</h3><p><a href=https://kubernetes.io/zh/docs/tasks/manage-daemon/update-daemon-set/>对 DaemonSet 执行滚动更新</a>，本地控制DO的集群，默认会超时，无法apply对象，提示</p><pre tabindex=0><code>E1123 19:37:27.679680     511 request.go:1027] Unexpected error when reading response body: net/http: request canceled (Client.Timeout or context cancellation while reading body)
error: unexpected error when reading response body. Please retry. Original error: net/http: request canceled (Client.Timeout or context cancellation while reading body)
# 或者 
Unable to connect to the server: net/http: TLS handshake timeout
</code></pre><p>增加超时时间设置<code>k apply -f xxx.yaml --request-timeout="50"</code>，还可以指定log级别<code>-v=6</code>，参考<a href=https://www.mtioutput.com/entry/kubectl-timeout-error>【Kubernetes】Client.Timeout or context cancellation while reading bodyというエラーの解消方法</a></p><ul><li>设置DaemonSet内容，查看更新策略<code>kubectl get ds/fluentd-elasticsearch -n kube-system -o go-template='{{.spec.updateStrategy.type}}{{"\n"}}'</code> 查看方式也可以为 <code>-o yaml</code></li><li>对 RollingUpdate DaemonSet 的 <code>.spec.template</code> 的任何更新都将触发滚动更新。例如增加了cpu和内存的限制</li></ul><p>以下方式都可以更新——</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># 声明式命令</span>
</span></span><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset-update.yaml
</span></span><span style=display:flex><span><span style=color:#75715e># 指令式命令 Edit a resource from the default editor.</span>
</span></span><span style=display:flex><span>kubectl edit ds/fluentd-elasticsearch -n kube-system <span style=color:#75715e>#打开文件编辑，保持。远程不建议这样操作</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 只更新容器镜像</span>
</span></span><span style=display:flex><span>kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch<span style=color:#f92672>=</span>quay.io/fluentd_elasticsearch/fluentd:v2.6.0 -n kube-system
</span></span></code></pre></div><p>更新结果类似——</p><pre tabindex=0><code>myu@Gebitang:~$ k apply -f fluentd-daemonset-update.yaml
daemonset.apps/fluentd-elasticsearch configured
myu@Gebitang:~$ kubectl rollout status ds/fluentd-elasticsearch -n kube-system
Waiting for daemon set &#34;fluentd-elasticsearch&#34; rollout to finish: 1 out of 3 new pods have been updated...
Waiting for daemon set &#34;fluentd-elasticsearch&#34; rollout to finish: 1 out of 3 new pods have been updated...
Waiting for daemon set &#34;fluentd-elasticsearch&#34; rollout to finish: 2 out of 3 new pods have been updated...
Waiting for daemon set &#34;fluentd-elasticsearch&#34; rollout to finish: 2 out of 3 new pods have been updated...
Waiting for daemon set &#34;fluentd-elasticsearch&#34; rollout to finish: 2 out of 3 new pods have been updated...
Waiting for daemon set &#34;fluentd-elasticsearch&#34; rollout to finish: 2 of 3 updated pods are available...
daemon set &#34;fluentd-elasticsearch&#34; successfully rolled out
</code></pre><p>从名字空间中删除 DaemonSet：<code>kubectl delete ds fluentd-elasticsearch -n kube-system</code></p><h3 id=管理内存cpu配额>管理内存、CPU、配额</h3><p><a href=https://kubernetes.io/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>管理内存</a>，提示<code>looking up service account default-mem-example/default: serviceaccount "default" not found</code>，意味着没有服务账号(默认空间有默认的账号，但新创建的namespace下没有服务账户)，可以先执行任务<a href=https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-service-account/>为pod配置服务账户</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f - <span style=color:#e6db74>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#e6db74>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>kind: ServiceAccount
</span></span></span><span style=display:flex><span><span style=color:#e6db74>metadata:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  name: default
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  namespace: default-mem-example
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 然后再在对应空间下创建pod即可</span>
</span></span></code></pre></div><p>执行删除namespace的命令后<code>k delete namespace default-mem-example</code>，一直没有返回结果，Ctrl+C强制结束，查看状态显示为<code>Terminating</code>，参考<a href=https://stackoverflow.com/questions/52369247/namespace-stuck-as-terminating-how-i-removed-it>Namespace &ldquo;stuck&rdquo; as Terminating, How I removed it</a>，当前空间下还有其他资源？<code>kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n default-mem-example</code>，删除对应的资源后，状态依然为<code>Terminating</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>geb@Gebitang:~$ k delete limitranges --namespace<span style=color:#f92672>=</span>default-mem-example
</span></span><span style=display:flex><span>error: resource<span style=color:#f92672>(</span>s<span style=color:#f92672>)</span> were provided, but no name was specified
</span></span><span style=display:flex><span>geb@Gebitang:~$ k delete limitrange/mem-limit-range --namespace<span style=color:#f92672>=</span>default-mem-example
</span></span><span style=display:flex><span>limitrange <span style=color:#e6db74>&#34;mem-limit-range&#34;</span> deleted
</span></span><span style=display:flex><span>geb@Gebitang:~$ k delete serviceaccount/default --namespace<span style=color:#f92672>=</span>default-mem-example
</span></span><span style=display:flex><span>serviceaccount <span style=color:#e6db74>&#34;default&#34;</span> deleted
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看空间信息 k get namespace default-mem-example -o json</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;apiVersion&#34;</span>: <span style=color:#e6db74>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;kind&#34;</span>: <span style=color:#e6db74>&#34;Namespace&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;creationTimestamp&#34;</span>: <span style=color:#e6db74>&#34;2021-11-23T07:17:26Z&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;deletionTimestamp&#34;</span>: <span style=color:#e6db74>&#34;2021-11-23T07:55:36Z&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;labels&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;kubernetes.io/metadata.name&#34;</span>: <span style=color:#e6db74>&#34;default-mem-example&#34;</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;default-mem-example&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;resourceVersion&#34;</span>: <span style=color:#e6db74>&#34;2161742&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;uid&#34;</span>: <span style=color:#e6db74>&#34;7f0fd528-b681-45f7-887e-49820024333e&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;spec&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;finalizers&#34;</span>: [
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;kubernetes&#34;</span>
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;status&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;phase&#34;</span>: <span style=color:#e6db74>&#34;Terminating&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>调用api，将finalizers字段置空即可：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># 使用proxy，然后调用http server api</span>
</span></span><span style=display:flex><span>NAMESPACE<span style=color:#f92672>=</span>your-rogue-namespace
</span></span><span style=display:flex><span>kubectl proxy &amp;
</span></span><span style=display:flex><span>kubectl get namespace $NAMESPACE -o json |jq <span style=color:#e6db74>&#39;.spec = {&#34;finalizers&#34;:[]}&#39;</span> &gt;temp.json
</span></span><span style=display:flex><span>curl -k -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 直接调用，将上述json内容的finalizers数组置为空，保存为 default-mem-example.json， 调用</span>
</span></span><span style=display:flex><span> k replace --raw <span style=color:#e6db74>&#34;/api/v1/namespaces/default-mem-example/finalize&#34;</span> -f default-mem-example.json
</span></span><span style=display:flex><span><span style=color:#75715e># 再查看空间时，此空间被彻底删除</span>
</span></span></code></pre></div><p>其他资源类似</p><h2 id=start-over>start over</h2><h3 id=虚拟机环境>虚拟机环境</h3><ul><li>centOS7 <a href=http://isoredirect.centos.org/centos/7/isos/x86_64/>http://isoredirect.centos.org/centos/7/isos/x86_64/</a></li><li>debian11 <a href=https://www.debian.org/distrib/netinst>https://www.debian.org/distrib/netinst</a></li><li>ubuntu20.04 <a href=https://ubuntu.com/download/server>https://ubuntu.com/download/server</a></li></ul><p><strong>其他事项</strong></p><ul><li>旧的vmware player无法支撑debian11版本，重新下载安装player 16，分别创建三个虚拟机系统</li><li>国内镜像源：阿里 <a href=https://developer.aliyun.com/mirror/>https://developer.aliyun.com/mirror/</a>， 清华 <a href=https://mirrors.tuna.tsinghua.edu.cn/>https://mirrors.tuna.tsinghua.edu.cn/</a>， 163 <a href=http://mirrors.163.com/>http://mirrors.163.com/</a> ， 中科大 <a href=https://mirrors.ustc.edu.cn/>https://mirrors.ustc.edu.cn/</a></li><li>不同类型的镜像介绍： <a href=https://superuser.com/questions/968889/what-is-the-difference-between-live-bin-minimal-and-netinstall-versions-of-ce>Differences between distribution types</a><ul><li><strong>Live</strong> 类似可直接使用的USB系统</li><li><strong>Bin</strong> 桌面版。包含了GUI环境和其他软件</li><li><strong>Minimal</strong> 不包含GUI的版本</li><li><strong>Netinstall</strong> 需要从镜像站下载安装的版本</li></ul></li></ul><p>纯手工搭建还是比较费劲，添加镜像源之后通过命令行搭建比较靠谱。</p><ul><li><a href=https://docs.docker.com/engine/install/debian/>安装docker环境</a>： 配置好源之后；安装工具；添加GPG key；添加stable源；安装8个包</li><li>安装三件套<code>kubeadm kubelet kubectl</code></li><li>先确保kubelet<a href=https://stackoverflow.com/questions/52119985/kubeadm-init-shows-kubelet-isnt-running-or-healthy>正常启动</a></li><li>执行<code>sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers --service-cidr=10.96.0.0/12 --pod-network-cidr=192.168.0.0/16 --v=5</code>开始安装</li><li>安装成功后，master节点会显示为NotReady状态，查看后因为没有CNI插件，可以安装<a href=https://github.com/flannel-io/flannel#deploying-flannel-manually>Deploying flannel manually</a>，稍后恢复为ready状态。或者使用<a href=https://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises>Calico插件</a></li><li>确保宿主机上服务正常启动，如下操作</li><li>确保swap关闭<code>sudo swapoff -a</code></li><li>join后，部署kube-proxy时提示找不到<code>/run/systemd/resolve/resolv.conf</code>文件，从master节点复制一份</li><li>更多相关信息，参考<a href=https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>官方手册</a></li></ul><pre tabindex=0><code># work on ubuntu, debian, and centos 7
# create file /etc/docker/daemon.json with 
{
    &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;]
}
# do the following commands
 sudo systemctl daemon-reload
 sudo systemctl restart docker
 sudo systemctl restart kubelet
</code></pre><p>基本上就是踩完一遍坑就可以安装成功了。</p><h4 id=systemd-vs-cgroupfs>systemd vs cgroupfs</h4><p>kubernetes为什么要修改使用systemd？</p><p>Kubernetes 现在推荐使用 systemd 来代替 cgroupfs
因为systemd是Kubernetes自带的cgroup管理器，负责为每个进程分配cgroups
但docker的cgroup driver默认是cgroupfs，这样就同时运行有两个cgroup控制管理器
当资源有压力的情况时，有可能出现不稳定的情况。</p><p>如果不修改配置，会在kubeadm init时有提示:</p><pre tabindex=0><code>[WARNING IsDockerSystemdCheck]: detected &#34;cgroupfs&#34; as the Docker cgroup driver. 
The recommended driver is &#34;systemd&#34;. 
Please follow the guide at https://kubernetes.io/docs/setup/cri/
</code></pre><p>但<a href="https://www.ibm.com/docs/zh/cloud-private/3.2.0?topic=iu-log-flooding-when-using-systemd-as-cgroup-driver">使用 systemd 作为 cgroup 驱动程序可能导致日志泛滥</a></p><p>不影响容器指标，只需配置 rsyslog 来忽略此类泛滥的日志， 在 <code>/etc/rsyslog.conf</code> 或 <code>/etc/rsyslog.d/*.conf</code>中的rule部分下配置——</p><pre tabindex=0><code>rawmsg, contains, &#34;libcontainer&#34; ~

# Log all kernel messages to the console.
# Logging much else clutters up the screen.
#kern.*                                                 /dev/console

# Log anything (except mail) of level info or higher.
# Don&#39;t log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages
</code></pre><ul><li>每个规则行由两部分组成，selector部分和action部分，这两部分由一个或多个空格或tab分隔，selector部分指定源和日志等级，action部分指定对应的操作。</li><li>selector也由两部分组成，设施和优先级，由点号.分隔。第一部分为消息源或称为日志设施，第二部分为日志级别。</li></ul><p><code>rawmsg, contains, "libcontainer" ~</code> 波浪线表示discard丢弃。<a href=https://superuser.com/questions/1269643/why-does-mean-discard-the-messages-that-were-matched-in-the-previous-line>参考</a>，<a href=https://www.rsyslog.com/doc/master/configuration/actions.html#discard-stop>官方说明</a></p><blockquote><ol><li>Note that in legacy configuration the tilde character “~” can also be used instead of the word “stop”.</li><li><code>rawmsg</code> is the message as it is received from the network. It has the syslog header, e.g. timestamp and tag. <code>msg</code> is just the syslog payload (the MSG field from RFC3164 and RFC5424). If your senders are RFC-compliant, both should differ.</li></ol></blockquote><h3 id=纯手工搭建环境>纯手工搭建环境</h3><p>目前只有一台ubuntu 20.04的server机器。</p><p>先收工<a href=https://docs.docker.com/engine/install/ubuntu/>安装docker环境</a>，从<a href=https://download.docker.com/linux/ubuntu/dists/focal/pool/stable/amd64/>下载页面</a>下载docker-ce docker-ce-cli containerd.io</p><p><strong>containerd.io</strong> : daemon <a href=https://containerd.io/>containerd</a>. It <a href=https://containerd.io/docs/getting-started/>works</a> independently on the docker packages, and it is required by the docker packages.</p><blockquote><p>containerd is available as a daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond.</p></blockquote><p><strong>docker-ce-cli</strong> : command line interface for docker engine, community edition</p><p><strong>docker-ce</strong> : <a href=https://docs.docker.com/get-docker/>docker</a> engine, community edition. Requires docker-ce-cli.</p><blockquote><p><a href=https://download.docker.com/linux/ubuntu/dists/focal/pool/stable/amd64/containerd.io_1.4.12-1_amd64.deb>containerd.io_1.4.12-1_amd64.deb</a> 2021-12-11 23:03:31 22.6 MiB<br><a href=https://download.docker.com/linux/ubuntu/dists/focal/pool/stable/amd64/docker-ce-cli_20.10.12~3-0~ubuntu-focal_amd64.deb>docker-ce-cli_20.10.12<del>3-0</del>ubuntu-focal_amd64.deb</a> 2021-12-13 14:38:46 38.8 MiB<br><a href=https://download.docker.com/linux/ubuntu/dists/focal/pool/stable/amd64/docker-ce_20.10.12~3-0~ubuntu-focal_amd64.deb>docker-ce_20.10.12<del>3-0</del>ubuntu-focal_amd64.deb</a> 2021-12-13 14:38:48 20.3 MiB</p></blockquote><p>注意安装顺序——安装完成后会自动创建对应的systemd services</p><pre tabindex=0><code>➜  deps sudo dpkg -i containerd.io_1.4.12-1_amd64.deb
[sudo] password for geb:
Selecting previously unselected package containerd.io.
(Reading database ... 109132 files and directories currently installed.)
Preparing to unpack containerd.io_1.4.12-1_amd64.deb ...
Unpacking containerd.io (1.4.12-1) ...
Setting up containerd.io (1.4.12-1) ...
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /lib/systemd/system/containerd.service.
Processing triggers for man-db (2.9.1-1) ...
➜  deps sudo dpkg -i docker-ce-cli_20.10.12_3-0_ubuntu-focal_amd64.deb
Selecting previously unselected package docker-ce-cli.
(Reading database ... 109160 files and directories currently installed.)
Preparing to unpack docker-ce-cli_20.10.12_3-0_ubuntu-focal_amd64.deb ...
Unpacking docker-ce-cli (5:20.10.12~3-0~ubuntu-focal) ...
Setting up docker-ce-cli (5:20.10.12~3-0~ubuntu-focal) ...
Processing triggers for man-db (2.9.1-1) ...
➜  deps sudo dpkg -i docker-ce_20.10.12_3-0_ubuntu-focal_amd64.deb
(Reading database ... 109358 files and directories currently installed.)
Preparing to unpack docker-ce_20.10.12_3-0_ubuntu-focal_amd64.deb ...
Unpacking docker-ce (5:20.10.12~3-0~ubuntu-focal) over (5:20.10.12~3-0~ubuntu-focal) ...
Setting up docker-ce (5:20.10.12~3-0~ubuntu-focal) ...
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /lib/systemd/system/docker.service.
Created symlink /etc/systemd/system/sockets.target.wants/docker.socket → /lib/systemd/system/docker.socket.
Processing triggers for systemd (245.4-4ubuntu3.15) ...
</code></pre><p>配置安装源(本页面后续内容)后，<a href=https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>安装 kubeadm、kubelet 和 kubectl</a></p><ul><li>kubeadm：用来初始化集群的指令。</li><li>kubelet：在集群中的每个节点上用来启动 Pod 和容器等。</li><li>kubectl：用来与集群通信的命令行工具。</li></ul><pre tabindex=0><code>sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

#算上以来文件，实际上安装了8个应用
conntrack cri-tools ebtables kubeadm kubectl kubelet kubernetes-cni socat 
</code></pre><p>执行初始化动作<code>sudo kubeadm init --v=5</code>可以看到更详细的安装信息，根据log信息可以看到大致执行了哪些动作——</p><p>下载clash，<code>gunzip clashxx.gz</code>解压；下载配置文件；<a href=https://github.com/Dreamacro/clash/issues/854>下载mmdb</a>；启动<code> ./clash -d conf -f clash.yml</code>走代理下载镜像；关闭swap <code>sudo swapoff -a</code> 还是提示超时。<a href=https://zhuanlan.zhihu.com/p/46341911>参考</a></p><p>执行<code>kubeadm config images list</code>获取需要安装的镜像列表，然后从国内站点拉取。更方便的方式是init时指定repository的地址<code>sudo kubeadm init --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers</code>，更多<a href=https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/>参数参考</a></p><ul><li>&ndash;pod-network-cidr string， 指明 pod 网络可以使用的 IP 地址段。如果设置了这个参数，控制平面将会为每一个节点自动分配 CIDRs。</li><li>&ndash;service-cidr string 默认值：&ldquo;10.96.0.0/12&rdquo;。为服务的虚拟 IP 地址另外指定 IP 地址段</li></ul><p><a href=https://www.jianshu.com/p/71220cbcf86e>关于CIDR</a>——Classless Inter-Domain Routing, 无类域间路由。基于变长子网掩码(<a href=https://www.techtarget.com/searchnetworking/definition/variable-length-subnet-mask>VLSM</a>)，举例，IP4由32位组成，则<code>192.255.255.255/12</code>前12位是地址的网络部分，而最后20位是主机地址。<a href=https://www.ipaddressguide.com/cidr>计算器</a>可计算合适的ip范围</p><p>可避免使用固定的私有地址——RFC1918规定的三类私有地址如下：</p><ul><li>A类：10.0.0.0 - 10.255.255.255（10.0.0.0/8）</li><li>B类：172.16.0.0 - 172.31.255.255（172.16.0.0/12）</li><li>C类：192.168.0.0 - 192.168.255.255（192.168.0.0/16）</li></ul><pre tabindex=0><code>I0116 12:34:31.850670   95794 checks.go:851] image exists: k8s.gcr.io/kube-apiserver:v1.23.1
	Unfortunately, an error has occurred:
		timed out waiting for the condition

	This error is likely caused by:
		- The kubelet is not running
		- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

	If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
		- &#39;systemctl status kubelet&#39;
		- &#39;journalctl -xeu kubelet&#39;

	Additionally, a control plane component may have crashed or exited when started by the container runtime.
	To troubleshoot, list all containers using your preferred container runtimes CLI.

	Here is one example how you may list all Kubernetes containers running in docker:
		- &#39;docker ps -a | grep kube | grep -v pause&#39;
		Once you have found the failing container, you can inspect its logs with:
		- &#39;docker logs CONTAINERID&#39;
</code></pre><p><a href=https://stackoverflow.com/questions/52119985/kubeadm-init-shows-kubelet-isnt-running-or-healthy>kubeadm init shows kubelet isn&rsquo;t running or healthy</a></p><p>提示Port 6443 is in use。需要先执行<code>sudo kubeadm reset</code>，重新执行init命令。终于OK了</p><pre tabindex=0><code>[certs] Using certificateDir folder &#34;/etc/kubernetes/pki&#34;
...
..
.
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.10.101:6443 --token sb4nsd.o4c2svxc6ey18vzv \
	--discovery-token-ca-cert-hash sha256:a21fa42fb12f65c33dab7dd37cc2341c07ba4b90675500a76b52a39e1507e082
</code></pre><p>默认添加的node节点没有roles的描述信息，可以通过<code>label</code>命令指定或重新，<a href=https://stackoverflow.com/questions/48854905/how-to-add-roles-to-nodes-in-kubernetes>参考</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl label nodes &lt;your_node&gt; kubernetes.io/role<span style=color:#f92672>=</span>&lt;your_label&gt;
</span></span><span style=display:flex><span>kubectl label --overwrite nodes &lt;your_node&gt; kubernetes.io/role<span style=color:#f92672>=</span>&lt;your_new_label&gt;
</span></span></code></pre></div><p><a href=https://stackoverflow.com/questions/51126164/how-do-i-find-the-join-command-for-kubeadm-on-the-master>worker节点加入集群</a></p><h3 id=wsl2-时间同步>wsl2 时间同步</h3><p>按照<a href=https://auth0.com/blog/kubernetes-tutorial-step-by-step-introduction-to-basic-concepts/>Kubernetes Tutorial - Step by Step Introduction to Basic Concepts</a>介绍在DO上创建免费的k8s集群，在wsl2里进行访问时提示错误：<code>Unable to connect to the server: x509: certificate has expired or is not yet valid: current time 2021-11-09T02:59:51+08:00 is before 2021-11-09T07:47:25Z</code>查看本地时间，显示比实际实际慢</p><p>执行<code>sudo hwclock -s </code>同步时间依然没有变化，官方<a href=https://github.com/microsoft/WSL/issues/4149>issue 4149</a>中提供了临时解决方案——netdate。立即生效，不影响任何进程</p><pre tabindex=0><code># 按照ntpdate set the date and time via  Network Time Protocol (NTP)
sudo apt install ntpdate
# 同步时间
sudo ntpdate -sb time.nist.gov
# 或执行
sudo ntpdate time.windows.com

myu@Gebitang:~$ sudo ntpdate time.windows.com
 9 Nov 16:09:43 ntpdate[1727]: step time server 52.231.114.183 offset 46499.066385 sec
</code></pre><p>同步时间之后，可以跟集群正常交互</p><h3 id=kubernetes-tutorial>Kubernetes Tutorial</h3><p>上面提到的文章有点老，遇到的问题记录一下——</p><ul><li>通过promote code注册DO需要手动提交个ticket，否则无法创建k8s集群，提示droplet超限</li><li>部署<code>NGINX ingress controller</code>的yaml文件地址已失效，参考<a href=https://kubernetes.github.io/ingress-nginx/deploy/#digital-ocean>ingress-nginx</a>中的说明，使用<code>https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.4/deploy/static/provider/do/deploy.yaml</code>文件</li></ul><pre tabindex=0><code>myu@Gebitang:~$ k get pods -n ingress-nginx
NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-n2dgj        0/1     Completed   0          87s
ingress-nginx-admission-patch-jldqd         0/1     Completed   2          86s
ingress-nginx-controller-5c8d66c76d-fgvpb   1/1     Running     0          90s
</code></pre><ul><li>最终部署完service之后，获取外网ip的步骤也失效了，通过<code>k get svc -n ingress-nginx</code>可以看到外网ip地址，尽管访问服务时提示404，至少表示nginx启动起来了</li></ul><h3 id=k8s中的crd开发>K8S中的CRD开发</h3><p>CRD: <code>CustomResourceDefinition</code>, Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.</p><p>官方<a href=https://github.com/kubernetes/sample-controller>sample-controller</a>有针对各个时期的版本说明。</p><p><a href=http://bingerambo.com/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91>2021-05-04版本</a>流程。依赖<a href=https://github.com/kubernetes/code-generator>code-generator</a>，也可以参考<a href=http://ljchen.net/2019/06/14/K8s-client%E4%BB%A3%E7%A0%81%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90/>K8s Client代码自动生成</a></p><p>使用代码生成器生成的api兼容标准的k8s api，操作动作相同。</p><p>流程概述——自定义资源文件之后，利用<code>code-generator</code>生成客户端代码（处理成类似client-go封装标准资源的逻辑，例如可以get、list、watch等）</p><p>第一、 生成框架代码结构。</p><ul><li>最顶层的register.go定义了groupName；</li><li>在版本号目录下又有三个文件：<ul><li>doc.go定义了版本的包名；</li><li>types.go定义资源的数据结构；</li><li>register.go真正用于注册该版本下的资源。</li></ul></li></ul><p>第二、 使用<code>code-generator</code>提供的脚本生成客户端代码，后续自己的controller逻辑就可以直接调用这个客户端代码。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>~/gopath/src/k8s.io/project&gt; tree
</span></span><span style=display:flex><span>.
</span></span><span style=display:flex><span>└── pkg
</span></span><span style=display:flex><span>    └── apis
</span></span><span style=display:flex><span>        └── controller
</span></span><span style=display:flex><span>            ├── register.go
</span></span><span style=display:flex><span>            └── v1
</span></span><span style=display:flex><span>                ├── doc.go
</span></span><span style=display:flex><span>                ├── register.go
</span></span><span style=display:flex><span>                └── types.go
</span></span><span style=display:flex><span><span style=color:#75715e># 生成代码   </span>
</span></span><span style=display:flex><span>~$ pwd
</span></span><span style=display:flex><span>~$ /gopath/src/k8s.io
</span></span><span style=display:flex><span>~$ $GOPATH/src/k8s.io/code-generator/generate-groups.sh all <span style=color:#ae81ff>\ </span>
</span></span><span style=display:flex><span>   k8s.io/project/pkg/generated <span style=color:#ae81ff>\ </span> <span style=color:#75715e># 注意路径</span>
</span></span><span style=display:flex><span>   k8s.io/project/pkg/apis <span style=color:#ae81ff>\ </span> 
</span></span><span style=display:flex><span>   controller:v1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Generating deepcopy funcs
</span></span><span style=display:flex><span>Generating clientset <span style=color:#66d9ef>for</span> controller:v1 at k8s.io/project/pkg/generated/clientset
</span></span><span style=display:flex><span>Generating listers <span style=color:#66d9ef>for</span> controller:v1 at k8s.io/project/pkg/generated/listers
</span></span><span style=display:flex><span>Generating informers <span style=color:#66d9ef>for</span> controller:v1 at k8s.io/project/pkg/generated/informers             
</span></span></code></pre></div><h3 id=setup-with-dashboard>setup with dashboard</h3><p><a href=https://www.downloadkubernetes.com/>download binary</a></p><p><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/>Install kubectl on Linux</a></p><p>关于UI管理使用的<a href=https://github.com/kubernetes/dashboard/tree/master/docs>kubernetes/dashboard</a>，参考具体的<a href=https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md>doc: creating sample user</a></p><p>关于用户的步骤——</p><ul><li>基于RBAC的方式创建<code>ServiceAccount</code></li><li>将创建的SA进行<code>CluserRoleBinding</code></li></ul><p>示例中如果将namespace指定为<code>cluser-admin</code>，将拥有管理员权限。(通过<code>k get clusterroles</code>查看集群中都有哪些角色)</p><pre tabindex=0><code>chmod +x kubectl
mkdir -p ~/.local/bin/kubectl
mv ./kubectl ~/.local/bin/kubectl
# and then add ~/.local/bin/kubectl to $PATH
</code></pre><p>自动补全，自动完成Enable kubectl autocompletion</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>source &lt;<span style=color:#f92672>(</span>kubectl completion bash<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>alias k<span style=color:#f92672>=</span>kubectl
</span></span><span style=display:flex><span>complete -F __start_kubectl k
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#shell生效</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#39;source &lt;(kubectl completion bash)&#39;</span> &gt;&gt;~/.bashrc
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#39;alias kc=kubectl&#39;</span> &gt;&gt;~/.bashrc
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#39;complete -F __start_kubectl kc&#39;</span> &gt;&gt;~/.bashrc
</span></span></code></pre></div><p>使用<code>kind</code>创建集群 <a href=https://kind.sigs.k8s.io/docs/user/quick-start/>quick start</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># 创建一个3节点集群的配置文件</span>
</span></span><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt; EOF &gt; kind-3nodes.yaml
</span></span></span><span style=display:flex><span><span style=color:#e6db74>kind: Cluster
</span></span></span><span style=display:flex><span><span style=color:#e6db74>apiVersion: kind.x-k8s.io/v1alpha4
</span></span></span><span style=display:flex><span><span style=color:#e6db74>nodes:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  - role: control-plane
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  - role: worker
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  - role: worker
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 使用配置文件创建新的集群</span>
</span></span><span style=display:flex><span>kind create cluster --name k3s --config ./kind-3nodes.yaml
</span></span><span style=display:flex><span><span style=color:#75715e># 获取集群节点</span>
</span></span><span style=display:flex><span>kubectl get nodes
</span></span><span style=display:flex><span><span style=color:#75715e># 删除集群</span>
</span></span><span style=display:flex><span>kind delete cluster --name k3s
</span></span></code></pre></div><h3 id=wsldocker的限制>WSL+Docker的限制</h3><p><a href=https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/>WSL+Docker: Kubernetes on the Windows Desktop</a></p><p>从WSL2里的ubuntu20.04镜像里使用kind创建的k8s cluster，无法cluster的node的ip，网络不通。——这看起来是个已知的问题：<a href=https://docs.docker.com/desktop/windows/networking/>Networking features in Docker Desktop for Windows</a>——</p><ul><li><p><strong>There is no docker0 bridge on Windows</strong><br>Because of the way networking is implemented in Docker Desktop for Windows, you cannot see a docker0 interface on the host. This interface is actually within the virtual machine.</p></li><li><p><strong>I cannot ping my containers</strong><br>Docker Desktop for Windows can’t route traffic to Linux containers. However, you can ping the Windows containers.</p></li><li><p><strong>Per-container IP addressing is not possible</strong>
The docker (Linux) bridge network is not reachable from the Windows host. However, it works with Windows containers.</p></li></ul><p>跟着<a href=https://www.qikqiak.com/post/deploy-k8s-on-win-use-wsl2>演示</a>直到部署三个节点的cluster都正常，安装一个 Kubernetes Dashboard时(<code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.1/aio/deploy/recommended.yaml</code>)，<code>kubectl get all -n kubernetes-dashboard</code>命令显示，一直处于"ContainerCreating"状态。感觉应该是我本地网络原因？ <a href=https://github.com/kubernetes/dashboard/issues/2863>issue 2863</a></p><p>多个配置文件切换<a href=https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/>Configure Access to Multiple Clusters</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># online.config.yaml for pro, dev.config.yaml for dev</span>
</span></span><span style=display:flex><span>kubectl config --kubeconfig<span style=color:#f92672>=</span>online.config.yaml view
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># export</span>
</span></span><span style=display:flex><span>export KUBECONFIG<span style=color:#f92672>=</span>$KUBECONFIG:$HOME/.kube/config
</span></span><span style=display:flex><span>export KUBECONFIG_SAVED<span style=color:#f92672>=</span>$KUBECONFIG
</span></span><span style=display:flex><span>kubectl config view
</span></span><span style=display:flex><span><span style=color:#75715e># clean up </span>
</span></span><span style=display:flex><span>export KUBECONFIG<span style=color:#f92672>=</span>$KUBECONFIG_SAVED
</span></span></code></pre></div><pre tabindex=0><code>geb@Gebitang:~$ kubectl get all -n kubernetes-dashboard
NAME                                             READY   STATUS              RESTARTS   AGE
pod/dashboard-metrics-scraper-5594697f48-wgpst   0/1     ContainerCreating   0          14m
pod/kubernetes-dashboard-bc8f4fc6f-f2mqg         0/1     ContainerCreating   0          14m

NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/dashboard-metrics-scraper   ClusterIP   10.96.21.147   &lt;none&gt;        8000/TCP   14m
service/kubernetes-dashboard        ClusterIP   10.96.173.37   &lt;none&gt;        443/TCP    14m

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/dashboard-metrics-scraper   0/1     1            0           14m
deployment.apps/kubernetes-dashboard        0/1     1            0           14m

NAME                                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/dashboard-metrics-scraper-5594697f48   1         1         0       14m
replicaset.apps/kubernetes-dashboard-bc8f4fc6f         1         1         0       14m

#  kubectl get pods --all-namespaces
NAMESPACE              NAME                                                      READY   STATUS              RESTARTS   AGE
kube-system            coredns-558bd4d5db-57bh5                                  1/1     Running             0          16m
kube-system            coredns-558bd4d5db-sfsfp                                  1/1     Running             0          16m
kube-system            etcd-wslkindmultinodes-control-plane                      1/1     Running             0          17m
kube-system            kindnet-2k9fg                                             1/1     Running             0          16m
kube-system            kindnet-ssl4j                                             1/1     Running             0          16m
kube-system            kindnet-z5mgq                                             1/1     Running             0          16m
kube-system            kube-apiserver-wslkindmultinodes-control-plane            1/1     Running             0          17m
kube-system            kube-controller-manager-wslkindmultinodes-control-plane   1/1     Running             1          17m
kube-system            kube-proxy-b4r8b                                          1/1     Running             0          16m
kube-system            kube-proxy-mwlms                                          1/1     Running             0          16m
kube-system            kube-proxy-smhm5                                          1/1     Running             0          16m
kube-system            kube-scheduler-wslkindmultinodes-control-plane            1/1     Running             1          17m
kubernetes-dashboard   dashboard-metrics-scraper-5594697f48-wgpst                0/1     ContainerCreating   0          15m
kubernetes-dashboard   kubernetes-dashboard-bc8f4fc6f-f2mqg                      0/1     ContainerCreating   0          15m
local-path-storage     local-path-provisioner-547f784dff-dngw5                   1/1     Running             0          16m
</code></pre><p>需要先安装网络——</p><p><a href=https://github.com/kubernetes/dashboard/issues/2863#issuecomment-368785304>参考这个comment</a></p><pre tabindex=0><code># 可以将url内容保存到文件进行执行
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml
</code></pre><p>执行之后，<code>kubectl get all -n kubernetes-dashboard</code>可以看到对应的pod都正常运行起来了</p><pre tabindex=0><code>geb@Gebitang:~$ kubectl get all -n kubernetes-dashboard
NAME                                             READY   STATUS    RESTARTS   AGE
pod/dashboard-metrics-scraper-5594697f48-wgpst   1/1     Running   0          33m
pod/kubernetes-dashboard-bc8f4fc6f-f2mqg         1/1     Running   0          33m

NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/dashboard-metrics-scraper   ClusterIP   10.96.21.147   &lt;none&gt;        8000/TCP   33m
service/kubernetes-dashboard        ClusterIP   10.96.173.37   &lt;none&gt;        443/TCP    33m

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/dashboard-metrics-scraper   1/1     1            1           33m
deployment.apps/kubernetes-dashboard        1/1     1            1           33m

NAME                                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/dashboard-metrics-scraper-5594697f48   1         1         1       33m
replicaset.apps/kubernetes-dashboard-bc8f4fc6f         1         1         1       33m
</code></pre><p>proxy启动后，使用如下方式获取token，</p><pre tabindex=0><code># 创建一个新的 ServiceAccount
kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
EOF
# 将上面的 SA 绑定到系统的 cluster-admin 这个集群角色上
kubectl apply -f - &lt;&lt;EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF

# ----------actual result-------
geb@Gebitang:~$ kubectl apply -f - &lt;&lt;EOF
&gt; apiVersion: v1
&gt; kind: ServiceAccount
etadata:&gt; metadata:
&gt;   name: admin-user
namespac&gt;   namespace: kubernetes-dashboard
&gt; EOF
serviceaccount/admin-user created
geb@Gebitang:~$ kubectl apply -f - &lt;&lt;EOF
iVersio&gt; apiVersion: rbac.authorization.k8s.io/v1
&gt; kind: ClusterRoleBinding
&gt; metadata:
&gt;   name: admin-user
&gt; roleRef:
&gt;   apiGroup: rbac.authorization.k8s.io
&gt;   kind: ClusterRole
&gt;   name: cluster-admin
&gt; subjects:
&gt; - kind: ServiceAccount
&gt;   name: admin-user
&gt;   namespace: kubernetes-dashboard
&gt; EOF
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
geb@Gebitang:~$
geb@Gebitang:~$ kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &#39;{print $1}&#39;)
Name:         admin-user-token-d4mkv
Namespace:    kubernetes-dashboard
Labels:       &lt;none&gt;
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: b66a6322-81a3-4bec-af6a-c329c4702ef8

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1066 bytes
namespace:  20 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjdnaktTQ2Nuc1RfbGhWaWs3NHYtRzF2VFhIU2ZTX3g4aFBHUkNfYkROOU0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWQ0bWt2Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiNjZhNjMyMi04MWEzLTRiZWMtYWY2YS1jMzI5YzQ3MDJlZjgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.pSiZrLue_t9R8qXN7Dm6jaOblVnLr_qI0NvkL25N3TB78rvR3O2zIzZ0cJ5ylvl7hdxIRXoKQxZUvY8cCeFRosBdGp_gaevd_33vlLNfcj0UM3x_9IQeS4cqPlI0EkWcrVXbPbQQyCLKBSgaaCNQrijtiQXUgaK2eLh8FCmvFIIE5U2vs-GqlmTy1YNpGr28WjtML-bFvQIsL7BpZieWzBNb7VfnsNMRPjpmUUE6iurVApwZSNTesFcKQP-dW7trN0B8hI7SllRMN64rfUnNwZX58eI5esgicQ2STJ64WkqTRYbGs1up__Iz_xvoRYXQq4bN_SSVnCnLOtf8jpVuyA
geb@Gebitang:~$
</code></pre><p>访问<code>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</code>选择token方式，将上面获取到的token输入后，就可以看到UI页面</p><p>删除集群——</p><pre tabindex=0><code># 查找集群
kind get cluster 
# 删除现在的集群
kind delete cluster --name wslkindmultinodes
</code></pre><h2 id=archive>archive</h2><p>使用环境Windows。</p><h3 id=wsl2--kubernetes>WSL2 + Kubernetes</h3><p><a href=https://www.qikqiak.com/post/deploy-k8s-on-win-use-wsl2>在 Windows 下使用 WSL2 搭建 Kubernetes 集群</a></p><ul><li>安装商店里的<code>windows terminal</code></li><li>更新ubuntu的软件源</li></ul><pre tabindex=0><code>cp /etc/apt/sources.list /etc/apt/sources.list.bak
root@k8s:~# echo &#34;deb http://mirrors.aliyun.com/ubuntu/ focal main restricted
deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted
deb http://mirrors.aliyun.com/ubuntu/ focal universe
deb http://mirrors.aliyun.com/ubuntu/ focal-updates universe
deb http://mirrors.aliyun.com/ubuntu/ focal multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-updates multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted
deb http://mirrors.aliyun.com/ubuntu/ focal-security universe
deb http://mirrors.aliyun.com/ubuntu/ focal-security multiverse&#34; &gt; /etc/apt/sources.list
</code></pre><ul><li>kind可以从github上下载后手动安装——本身是go编译的二进制文件</li><li>但kind安装kubernetes集群时使用的是编译发布在hub.docker.com上的镜像，WSL2里遇到网络问题</li></ul><blockquote><p>This will bootstrap a Kubernetes cluster using a pre-built <a href=https://kind.sigs.k8s.io/docs/design/node-image>node image</a>. Prebuilt images are hosted at<a href=https://hub.docker.com/r/kindest/node/><code>kindest/node</code></a></p></blockquote><pre tabindex=0><code>root@Gebitang:/home/geb# kind create cluster --name wslk8s
Creating cluster &#34;wslk8s&#34; ...
 ✓ Ensuring node image (kindest/node:v1.21.1) 🖼
 ✓ Preparing nodes 📦
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
Set kubectl context to &#34;kind-wslk8s&#34;
You can now use your cluster with:

kubectl cluster-info --context kind-wslk8s

Thanks for using kind! 😊
</code></pre><p>重新使用<code>kubeadm</code>进行安装：</p><ul><li>需要先将apt-key.gpg证书导入<code>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -</code>。可以先将文件下载到本地，然后<code>cat apt-get.gpg | apt-key add -</code></li><li>添加镜像源到列表：在<code>/etc/apt/sources.list.d/</code>目录下创建<code>kubernetes.list</code>文件，内容为<code>deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main</code></li><li>执行<code>apt-get update && apt-get install -y kubeadm</code></li></ul><p>执行<code>kubeadm version</code>返回信息。</p><h3 id=errors-were-encountered-while-processing--ubuntu-advantage-tools>Errors were encountered while processing: ubuntu-advantage-tools</h3><p>ubuntu20.04执行<code>apt-get upgrade</code>时遇到类似上面的问题，看起来是个<a href=https://bugs.launchpad.net/ubuntu/+source/ubuntu-advantage-tools/+bug/1938097>bug</a>，官方提示修复方式：</p><ul><li>编辑<code>/var/lib/dpkg/info/ubuntu-advantage-tools.postinst</code></li><li>执行<code>dpkg --configure -a</code></li></ul><pre tabindex=0><code>I think the problem is in the postinst script, line 295:

    cloud_id=&#34;&#34;
    if command -v &#34;cloud-id&#34; &gt; /dev/null ; then
      cloud_id=$(cloud-id)
    fi

The third line should rather be:

      cloud_id=$(cloud-id || true)
</code></pre><p>根据<a href=https://edu.aliyun.com/roadmap/cloudnative>云原生技术基础</a>的<a href=https://edu.aliyun.com/lesson_1651_16894>第三课的demo</a></p><h3 id=前提环境>前提环境</h3><p>三件套：VirturalBox、kubectl、minikube。</p><ul><li><p>安装 VirtualBox： <a href=https://www.virtualbox.org/wiki/Downloads>https://www.virtualbox.org/wiki/Downloads</a></p></li><li><p>安装<a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-windows>kubectl on windows</a></p></li></ul><p>下载、添加PATH、验证版本</p><ol><li><p>Download the latest release v1.18.0 from <a href=https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/windows/amd64/kubectl.exe>this link</a>.</p><p>Or if you have <code>curl</code> installed, use this command:</p><pre tabindex=0><code>curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/windows/amd64/kubectl.exe
</code></pre><p>To find out the latest stable version (for example, for scripting), take a look at <a href=https://storage.googleapis.com/kubernetes-release/release/stable.txt>https://storage.googleapis.com/kubernetes-release/release/stable.txt</a>.</p></li><li><p>Add the binary in to your PATH.</p></li><li><p>Test to ensure the version of <code>kubectl</code> is the same as downloaded:</p><pre tabindex=0><code>kubectl version --client
</code></pre></li></ol><ul><li>安装minikube，<a href=https://kubernetes.io/docs/tasks/tools/install-minikube/>原生安装</a>，<a href=https://developer.aliyun.com/article/221687>aliyun: Minikube - Kubernetes本地实验环境</a></li></ul><p>先进行系统检测<code>systeminfo</code></p><pre tabindex=0><code>Hyper-V Requirements:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.
Hyper-V 要求:     已检测到虚拟机监控程序。将不显示 Hyper-V 所需的功能。
</code></pre><hr><p>启动命令： <code>minikube start --driver virtualbox</code></p><pre tabindex=0><code>D:\&gt;minikube start --driver=virtualbox
* Microsoft Windows 10 Education 10.0.18363 Build 18363 上的 minikube v1.12.3
* 根据现有的配置文件使用 virtualbox 驱动程序
* Starting control plane node minikube in cluster minikube
* Creating virtualbox VM (CPUs=4, Memory=6000MB, Disk=20000MB) ...
! StartHost failed, but will try again: creating host: create: precreate: This computer is running Hyper-V. VirtualBox won&#39;t boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check)
* Creating virtualbox VM (CPUs=4, Memory=6000MB, Disk=20000MB) ...
* Failed to start virtualbox VM. &#34;minikube start&#34; may fix it: creating host: create: precreate: This computer is running Hyper-V. VirtualBox won&#39;t boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check)
*
E0821 17:24:38.091674   11324 exit.go:76] &amp;{ID:VBOX_HYPERV_64_BOOT Err:This computer is running Hyper-V. VirtualBox won&#39;t boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check)
precreate
k8s.io/minikube/pkg/minikube/machine.(*LocalClient).Create
        /app/pkg/minikube/machine/client.go:225
k8s.io/minikube/pkg/minikube/machine.timedCreateHost.func2
        /app/pkg/minikube/machine/start.go:188
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1373
create
k8s.io/minikube/pkg/minikube/machine.timedCreateHost
        /app/pkg/minikube/machine/start.go:197
k8s.io/minikube/pkg/minikube/machine.createHost
        /app/pkg/minikube/machine/start.go:163
k8s.io/minikube/pkg/minikube/machine.StartHost
        /app/pkg/minikube/machine/start.go:86
k8s.io/minikube/pkg/minikube/node.startHost
        /app/pkg/minikube/node/start.go:401
k8s.io/minikube/pkg/minikube/node.startMachine
        /app/pkg/minikube/node/start.go:345
k8s.io/minikube/pkg/minikube/node.Provision
        /app/pkg/minikube/node/start.go:234
k8s.io/minikube/cmd/minikube/cmd.provisionWithDriver
        /app/cmd/minikube/cmd/start.go:275
k8s.io/minikube/cmd/minikube/cmd.runStart
        /app/cmd/minikube/cmd/start.go:169
github.com/spf13/cobra.(*Command).execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:846
github.com/spf13/cobra.(*Command).ExecuteC
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
k8s.io/minikube/cmd/minikube/cmd.Execute
        /app/cmd/minikube/cmd/root.go:106
main.main
        /app/cmd/minikube/main.go:71
runtime.main
        /usr/local/go/src/runtime/proc.go:203
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1373
creating host
k8s.io/minikube/pkg/minikube/machine.createHost
        /app/pkg/minikube/machine/start.go:164
k8s.io/minikube/pkg/minikube/machine.StartHost
        /app/pkg/minikube/machine/start.go:86
k8s.io/minikube/pkg/minikube/node.startHost
        /app/pkg/minikube/node/start.go:401
k8s.io/minikube/pkg/minikube/node.startMachine
        /app/pkg/minikube/node/start.go:345
k8s.io/minikube/pkg/minikube/node.Provision
        /app/pkg/minikube/node/start.go:234
k8s.io/minikube/cmd/minikube/cmd.provisionWithDriver
        /app/cmd/minikube/cmd/start.go:275
k8s.io/minikube/cmd/minikube/cmd.runStart
        /app/cmd/minikube/cmd/start.go:169
github.com/spf13/cobra.(*Command).execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:846
github.com/spf13/cobra.(*Command).ExecuteC
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
k8s.io/minikube/cmd/minikube/cmd.Execute
        /app/cmd/minikube/cmd/root.go:106
main.main
        /app/cmd/minikube/main.go:71
runtime.main
        /usr/local/go/src/runtime/proc.go:203
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1373
Failed to start host
k8s.io/minikube/pkg/minikube/node.startMachine
        /app/pkg/minikube/node/start.go:347
k8s.io/minikube/pkg/minikube/node.Provision
        /app/pkg/minikube/node/start.go:234
k8s.io/minikube/cmd/minikube/cmd.provisionWithDriver
        /app/cmd/minikube/cmd/start.go:275
k8s.io/minikube/cmd/minikube/cmd.runStart
        /app/cmd/minikube/cmd/start.go:169
github.com/spf13/cobra.(*Command).execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:846
github.com/spf13/cobra.(*Command).ExecuteC
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
        /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
k8s.io/minikube/cmd/minikube/cmd.Execute
        /app/cmd/minikube/cmd/root.go:106
main.main
        /app/cmd/minikube/main.go:71
runtime.main
        /usr/local/go/src/runtime/proc.go:203
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1373 Advice:VirtualBox and Hyper-V are having a conflict. Use &#39;--driver=hyperv&#39; or disable Hyper-V using: &#39;bcdedit /set hypervisorlaunchtype off&#39; URL: Issues:[4051 4783] ShowIssueLink:false}
* [VBOX_HYPERV_64_BOOT] error provisioning host Failed to start host: creating host: create: precreate: This computer is running Hyper-V. VirtualBox won&#39;t boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check)
* 建议：VirtualBox and Hyper-V are having a conflict. Use &#39;--driver=hyperv&#39; or disable Hyper-V using: &#39;bcdedit /set hypervisorlaunchtype off&#39;
* 相关问题：
  - https://github.com/kubernetes/minikube/issues/4051
  - https://github.com/kubernetes/minikube/issues/4783
</code></pre><p>报错信息还是很清楚，运行时冲突：</p><ul><li>使用管理员权限执行<code>bcdedit /set hypervisorlaunchtype off</code>，成功后，还是提示相同的问题。</li><li>使用<code>--virtualbox-no-vtx-check</code> 提示这个<code>Error: unknown flag: --virtualbox-no-vtx-check</code> 可以See <code>minikube start --help' for usage.</code></li></ul><p>果然，现在的参数是<code>--no-vtx-check=true</code>。所以完整的参数应该是<code>minikube start --driver=virtualbox --no-vtx-check=true</code></p><p>不幸的时，还是遇到问题了</p><pre tabindex=0><code>PS D:\&gt; minikube start --driver=virtualbox --no-vtx-check=true
* Microsoft Windows 10 Education 10.0.18363 Build 18363 上的 minikube v1.12.3
* 根据现有的配置文件使用 virtualbox 驱动程序
* Starting control plane node minikube in cluster minikube
* Creating virtualbox VM (CPUs=4, Memory=6000MB, Disk=20000MB) ...
* 正在 Docker 19.03.12 中准备 Kubernetes v1.18.3…
* Unable to load cached images: loading cached images: transferring cached image: sudo test -d /var/lib/minikube/images &amp;&amp; sudo scp -t /var/lib/minikube/images &amp;&amp; sudo touch -d &#34;2020-08-21 17:31:20.5418176 +0800&#34; /var/lib/minikube/images/etcd_3.4.3-0: wait: remote command exited without exit status or exit signal
output:
...
stdout:

stderr:
fatal error: index out of range
runtime: panic before malloc heap initialized
</code></pre><p>执行<code>bcdedit /set hypervisorlaunchtype off</code>之后出现docker无法启动情况？管理员权限执行<code>bcdedit /Set {current} hypervisorlaunchtype auto</code>恢复初始值，执行<code>bcdedit</code>可看到全部配置信息。</p><p>自动提示链接为<a href=https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization-must-be-enabled>VIRTUALIZATION MUST BE ENABLED</a>，但检查是符合要求的。重启后再看吧。接着先往下学课程。</p><div class="prev-next-post pure-g"><div class=pure-u-1-24 style=text-align:left><a href=http://gebitang.com/post/how/harbor/><i class="fa fa-chevron-left"></i></a></div><div class=pure-u-10-24><nav class=prev><a href=http://gebitang.com/post/how/harbor/>harbor basic</a></nav></div><div class=pure-u-2-24>&nbsp;</div><div class=pure-u-10-24><nav class=next><a href=http://gebitang.com/post/how/kubernetes-monitor/>Prometheus And Grafana</a></nav></div><div class=pure-u-1-24 style=text-align:right><a href=http://gebitang.com/post/how/kubernetes-monitor/><i class="fa fa-chevron-right"></i></a></div></div><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var t,e=document.createElement("script");e.type="text/javascript",e.async=!0,t="gebitang",e.src="//"+t+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=http://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div><script src=http://gebitang.com/js/ui.js></script><script src=http://gebitang.com/js/menus.js></script><script>window.location.hostname!="localhost"&&(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","//www.google-analytics.com/analytics.js","ga"),ga("create","UA-34303694-1","auto"),ga("send","pageview"))</script><button onclick=topFunction() id=myBtn title="Go to top">Top</button></body></html>